{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9b1840",
   "metadata": {},
   "source": [
    "# Examples for packages/scikit-learn/index.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4c9c0",
   "metadata": {},
   "source": [
    "## Simple picture of the formal problem of machine learning\n",
    "\n",
    "<!--- plot_separator -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc916c",
   "metadata": {},
   "source": [
    "This example generates simple synthetic data points and shows a\n",
    "separating hyperplane on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 50 separable synthetic points\n",
    "X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, fit_intercept=True)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "xx = np.linspace(-1, 5, 10)\n",
    "yy = np.linspace(-1, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa83eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2 = np.meshgrid(xx, yy)\n",
    "Z = np.empty(X1.shape)\n",
    "for (i, j), val in np.ndenumerate(X1):\n",
    "    x1 = val\n",
    "    x2 = X2[i, j]\n",
    "    p = clf.decision_function([[x1, x2]])\n",
    "    Z[i, j] = p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.contour(\n",
    "    X1, X2, Z, [-1.0, 0.0, 1.0], colors=\"k\", linestyles=[\"dashed\", \"solid\", \"dashed\"]\n",
    ")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"Paired\")\n",
    "ax.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097c9bf",
   "metadata": {},
   "source": [
    "## linear_regression\n",
    "\n",
    "<!--- plot_linear_regression -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cf317",
   "metadata": {},
   "source": [
    "**A simple linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8dd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x from 0 to 30\n",
    "rng = np.random.default_rng()\n",
    "x = 30 * rng.random((20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = a*x + b with noise\n",
    "y = 0.5 * x + 1.0 + rng.normal(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b52d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y from the data\n",
    "x_new = np.linspace(0, 30, 100)\n",
    "y_new = model.predict(x_new[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15524303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_new, y_new)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b413c",
   "metadata": {},
   "source": [
    "## Plot 2D views of the iris dataset\n",
    "\n",
    "<!--- plot_iris_scatter -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd8072",
   "metadata": {},
   "source": [
    "Plot a simple scatter plot of 2 features of the iris dataset.\n",
    "\n",
    "Note that more elaborate visualization of this dataset is detailed\n",
    "in the {ref}`statistics` chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414afd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59878ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806dca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices of the features that we are plotting\n",
    "x_index = 0\n",
    "y_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = ticker.FuncFormatter(lambda i, *args: iris.target_names[int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target)\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5dc765",
   "metadata": {},
   "source": [
    "## Nearest-neighbor prediction on iris\n",
    "\n",
    "<!--- plot_iris_knn -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0615ff",
   "metadata": {},
   "source": [
    "Plot the decision boundary of nearest neighbor decision on iris, first\n",
    "with a single nearest neighbor, and then using 3 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
    "cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f794b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "# avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed647e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlabel(\"sepal length (cm)\")\n",
    "plt.ylabel(\"sepal width (cm)\")\n",
    "plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now, redo the analysis with 3 neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a951ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50862fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlabel(\"sepal length (cm)\")\n",
    "plt.ylabel(\"sepal width (cm)\")\n",
    "plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27597e91",
   "metadata": {},
   "source": [
    "## Plot fitting a 9th order polynomial\n",
    "\n",
    "<!--- plot_polynomial_regression -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8dbde",
   "metadata": {},
   "source": [
    "Fits data generated from a 9th order polynomial with model of 4th order\n",
    "and 9th order polynomials, to demonstrate that often simpler models are\n",
    "to be preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
    "cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)\n",
    "x = 2 * rng.random(100) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6931be",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda t: 1.2 * t**2 + 0.1 * t**3 - 0.4 * t**5 - 0.5 * t**9\n",
    "y = f(x) + 0.4 * rng.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x, y, s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a288af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting 4th and 9th order polynomials\n",
    "#\n",
    "# For this we need to engineer features: the n_th powers of x:\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x, y, s=4)\n",
    "\n",
    "X = np.array([x**i for i in range(5)]).T\n",
    "X_test = np.array([x_test**i for i in range(5)]).T\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "plt.plot(x_test, regr.predict(X_test), label=\"4th order\")\n",
    "\n",
    "X = np.array([x**i for i in range(10)]).T\n",
    "X_test = np.array([x_test**i for i in range(10)]).T\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "plt.plot(x_test, regr.predict(X_test), label=\"9th order\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"Fitting a 4th and a 9th order polynomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76065aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x, y, s=4)\n",
    "plt.plot(x_test, f(x_test), label=\"truth\")\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"Ground truth (9th order polynomial)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3879fc9",
   "metadata": {},
   "source": [
    "## Simple visualization and classification of the digits dataset\n",
    "\n",
    "<!--- plot_digits_simple_classif -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fce45",
   "metadata": {},
   "source": [
    "Plot the first few samples of the digits dataset and a 2D representation\n",
    "built using PCA, then do a simple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfcb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data: images of digits\n",
    "# -------------------------------\n",
    "#\n",
    "# Each data in a 8x8 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213575e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=\"binary\", interpolation=\"nearest\")\n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69db8a4",
   "metadata": {},
   "source": [
    "Plot a projection on the 2 first principal axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.figure()\n",
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(digits.data)\n",
    "plt.scatter(proj[:, 0], proj[:, 1], c=digits.target, cmap=\"Paired\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe1c52",
   "metadata": {},
   "source": [
    "Classify with Gaussian naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08155f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04056e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe92d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict the labels of the test data\n",
    "predicted = clf.predict(X_test)\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9726e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=\"binary\", interpolation=\"nearest\")\n",
    "\n",
    "    # label the image with the target value\n",
    "    if predicted[i] == expected[i]:\n",
    "        ax.text(0, 7, str(predicted[i]), color=\"green\")\n",
    "    else:\n",
    "        ax.text(0, 7, str(predicted[i]), color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the performance\n",
    "# ------------------------\n",
    "#\n",
    "# First print the number of correct matches\n",
    "matches = predicted == expected\n",
    "print(matches.sum())\n",
    "\n",
    "# The total number of data points\n",
    "print(len(matches))\n",
    "\n",
    "#  And now, the ratio of correct predictions\n",
    "matches.sum() / float(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b24adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdc50d",
   "metadata": {},
   "source": [
    "## A simple regression analysis on the California housing data\n",
    "\n",
    "<!--- plot_california_prediction -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42392079",
   "metadata": {},
   "source": [
    "Here we perform a simple regression analysis on the California housing\n",
    "data, exploring two types of regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63029d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a histogram of the quantity to predict: price\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(data.target)\n",
    "plt.xlabel(\"price ($100k)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174fed0",
   "metadata": {},
   "source": [
    "Print the joint histogram for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615693fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, feature_name in enumerate(data.feature_names):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.scatter(data.data[feature_name], data.target)\n",
    "    plt.ylabel(\"Price\", size=15)\n",
    "    plt.xlabel(feature_name, size=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db97ef",
   "metadata": {},
   "source": [
    "### Simple prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fad52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(expected, predicted)\n",
    "plt.plot([0, 8], [0, 8], \"--k\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel(\"True price ($100k)\")\n",
    "plt.ylabel(\"Predicted price ($100k)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc37be",
   "metadata": {},
   "source": [
    "Prediction with gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e463447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2edaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16672f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(expected, predicted)\n",
    "plt.plot([0, 5], [0, 5], \"--k\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel(\"True price ($100k)\")\n",
    "plt.ylabel(\"Predicted price ($100k)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the error rate\n",
    "print(f\"RMS: {np.sqrt(np.mean((predicted - expected) ** 2))!r} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb09dbb",
   "metadata": {},
   "source": [
    "## Measuring Decision Tree performance\n",
    "\n",
    "<!--- plot_measuring_performance -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5702d",
   "metadata": {},
   "source": [
    "Demonstrates overfit when testing on train set.\n",
    "\n",
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a model\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeRegressor().fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(data.data)\n",
    "expected = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff07ad8",
   "metadata": {},
   "source": [
    "Plot predicted as a function of expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e66be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(expected, predicted)\n",
    "plt.plot([0, 5], [0, 5], \"--k\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel(\"True price ($100k)\")\n",
    "plt.ylabel(\"Predicted price ($100k)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874bc47",
   "metadata": {},
   "source": [
    "Pretty much no errors!\n",
    "\n",
    "This is too good to be true: we are testing the model on the train\n",
    "data, which is not a measure of generalization.\n",
    "\n",
    "**The results are not valid**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50931c6",
   "metadata": {},
   "source": [
    "## linear_model_cv\n",
    "\n",
    "<!--- plot_linear_model_cv -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e2beb",
   "metadata": {},
   "source": [
    "Use the RidgeCV and LassoCV to set the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cross-validation score with the default hyper-parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dac32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Model in [Ridge, Lasso]:\n",
    "    model = Model()\n",
    "    print(f\"{Model.__name__}: {cross_val_score(model, X, y).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the cross-validation score as a function of alpha, the\n",
    "# strength of the regularization for Lasso and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50386b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, -1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Model in [Lasso, Ridge]:\n",
    "    scores = [cross_val_score(Model(alpha), X, y, cv=3).mean() for alpha in alphas]\n",
    "    plt.plot(alphas, scores, label=Model.__name__)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"cross validation score\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd4948",
   "metadata": {},
   "source": [
    "## pca\n",
    "\n",
    "<!--- plot_pca -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716399d",
   "metadata": {},
   "source": [
    "Demo PCA in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris data\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data in 2D\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90429161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "target_ids = range(len(iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1396ee",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "for i, c, label in zip(target_ids, \"rgbcmykw\", iris.target_names, strict=False):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=c, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e956c",
   "metadata": {},
   "source": [
    "## tSNE to visualize digits\n",
    "\n",
    "<!--- plot_tsne -->\n",
    "\n",
    "Here we use {class}`sklearn.manifold.TSNE` to visualize the digits\n",
    "datasets. Indeed, the digits are vectors in a 8\\*8 = 64 dimensional space.\n",
    "We want to project them in 2D for visualization. tSNE is often a good\n",
    "solution, as it groups and separates data points based on their local\n",
    "relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b39c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris data\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "# Take the first 500 data points: it's hard to see 1500 points\n",
    "X = digits.data[:500]\n",
    "y = digits.target[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform with a TSNE\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f312cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data in 2D\n",
    "X_2d = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "target_ids = range(len(digits.target_names))\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = \"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\", \"orange\", \"purple\"\n",
    "for i, c, label in zip(target_ids, colors, digits.target_names, strict=True):\n",
    "    plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], c=c, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d906d1",
   "metadata": {},
   "source": [
    "## Bias and variance of polynomial fit\n",
    "\n",
    "<!--- plot_bias_variance -->\n",
    "\n",
    "Demo overfitting, underfitting, and validation and learning curves with\n",
    "polynomial regression.\n",
    "\n",
    "Fit polynomes of different degrees to a dataset: for too small a degree,\n",
    "the model _underfits_, while for too large a degree, it overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ba8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_func(x, rng=None, error=0.5):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    return rng.normal(10 - 1.0 / (x + 0.1), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0745d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A polynomial regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686f70b",
   "metadata": {},
   "source": [
    "A simple figure to illustrate the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)\n",
    "x = 10 ** np.linspace(-2, 0, n_samples)\n",
    "y = generating_func(x, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3575c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.2, 1.2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e56f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"d = 1 (under-fit; high bias)\", \"d = 2\", \"d = 6 (over-fit; high variance)\"]\n",
    "degrees = [1, 2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3.5))\n",
    "fig.subplots_adjust(left=0.06, right=0.98, bottom=0.15, top=0.85, wspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(degrees):\n",
    "    ax = fig.add_subplot(131 + i, xticks=[], yticks=[])\n",
    "    ax.scatter(x, y, marker=\"x\", c=\"k\", s=50)\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax.plot(x_test, model.predict(x_test[:, np.newaxis]), \"-b\")\n",
    "\n",
    "    ax.set_xlim(-0.2, 1.2)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.set_xlabel(\"house size\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"price\")\n",
    "\n",
    "    ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "test_size = 0.4\n",
    "error = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d25784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample the data\n",
    "x = rng.random(n_samples)\n",
    "y = generating_func(x, rng=rng, error=error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c014a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, validation, and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the training and validation sets\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_train, y_train, color=\"red\", label=\"Training set\")\n",
    "plt.scatter(x_test, y_test, color=\"blue\", label=\"Test set\")\n",
    "plt.title(\"The data\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af67262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a validation curve\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d421e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(1, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3eff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter to vary is the \"degrees\" on the pipeline step\n",
    "# \"polynomialfeatures\"\n",
    "train_scores, validation_scores = validation_curve(\n",
    "    model,\n",
    "    x[:, np.newaxis],\n",
    "    y,\n",
    "    param_name=\"polynomialfeatures__degree\",\n",
    "    param_range=degrees,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a21a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean train error and validation error across folds\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(degrees, validation_scores.mean(axis=1), lw=2, label=\"cross-validation\")\n",
    "plt.plot(degrees, train_scores.mean(axis=1), lw=2, label=\"training\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"degree of fit\")\n",
    "plt.ylabel(\"explained variance\")\n",
    "plt.title(\"Validation curve\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c7ab6",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "\n",
    "Plot train and test error with an increasing number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A learning curve for d=1, 5, 15\n",
    "for d in [1, 5, 15]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree=d), LinearRegression())\n",
    "\n",
    "    from sklearn.model_selection import learning_curve\n",
    "\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        model, x[:, np.newaxis], y, train_sizes=np.logspace(-1, 0, 20)\n",
    "    )\n",
    "\n",
    "    # Plot the mean train error and validation error across folds\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(\n",
    "        train_sizes, validation_scores.mean(axis=1), lw=2, label=\"cross-validation\"\n",
    "    )\n",
    "    plt.plot(train_sizes, train_scores.mean(axis=1), lw=2, label=\"training\")\n",
    "    plt.ylim(ymin=-0.1, ymax=1)\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"number of train samples\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.title(f\"Learning curve (degree={d})\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2b332",
   "metadata": {},
   "source": [
    "## Other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d350320",
   "metadata": {},
   "source": [
    "### Tutorial Diagrams\n",
    "\n",
    "<!--- plot_ML_flow_chart -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81836e87",
   "metadata": {},
   "source": [
    "This script plots the flow-charts used in the scikit-learn tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle, Rectangle, Polygon, Arrow, FancyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base(box_bg=\"#CCCCCC\", arrow1=\"#88CCFF\", arrow2=\"#88FF88\", supervised=True):\n",
    "    fig = plt.figure(figsize=(9, 6), facecolor=\"w\")\n",
    "    ax = plt.axes((0, 0, 1, 1), xticks=[], yticks=[], frameon=False)\n",
    "    ax.set_xlim(0, 9)\n",
    "    ax.set_ylim(0, 6)\n",
    "\n",
    "    patches = [\n",
    "        Rectangle((0.3, 3.6), 1.5, 1.8, zorder=1, fc=box_bg),\n",
    "        Rectangle((0.5, 3.8), 1.5, 1.8, zorder=2, fc=box_bg),\n",
    "        Rectangle((0.7, 4.0), 1.5, 1.8, zorder=3, fc=box_bg),\n",
    "        Rectangle((2.9, 3.6), 0.2, 1.8, fc=box_bg),\n",
    "        Rectangle((3.1, 3.8), 0.2, 1.8, fc=box_bg),\n",
    "        Rectangle((3.3, 4.0), 0.2, 1.8, fc=box_bg),\n",
    "        Rectangle((0.3, 0.2), 1.5, 1.8, fc=box_bg),\n",
    "        Rectangle((2.9, 0.2), 0.2, 1.8, fc=box_bg),\n",
    "        Circle((5.5, 3.5), 1.0, fc=box_bg),\n",
    "        Polygon([[5.5, 1.7], [6.1, 1.1], [5.5, 0.5], [4.9, 1.1]], fc=box_bg),\n",
    "        FancyArrow(\n",
    "            2.3, 4.6, 0.35, 0, fc=arrow1, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "        FancyArrow(\n",
    "            3.75, 4.2, 0.5, -0.2, fc=arrow1, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "        FancyArrow(\n",
    "            5.5, 2.4, 0, -0.4, fc=arrow1, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "        FancyArrow(\n",
    "            2.0, 1.1, 0.5, 0, fc=arrow2, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "        FancyArrow(\n",
    "            3.3, 1.1, 1.3, 0, fc=arrow2, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "        FancyArrow(\n",
    "            6.2, 1.1, 0.8, 0, fc=arrow2, width=0.25, head_width=0.5, head_length=0.2\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if supervised:\n",
    "        patches += [\n",
    "            Rectangle((0.3, 2.4), 1.5, 0.5, zorder=1, fc=box_bg),\n",
    "            Rectangle((0.5, 2.6), 1.5, 0.5, zorder=2, fc=box_bg),\n",
    "            Rectangle((0.7, 2.8), 1.5, 0.5, zorder=3, fc=box_bg),\n",
    "            FancyArrow(\n",
    "                2.3, 2.9, 2.0, 0, fc=arrow1, width=0.25, head_width=0.5, head_length=0.2\n",
    "            ),\n",
    "            Rectangle((7.3, 0.85), 1.5, 0.5, fc=box_bg),\n",
    "        ]\n",
    "    else:\n",
    "        patches += [Rectangle((7.3, 0.2), 1.5, 1.8, fc=box_bg)]\n",
    "\n",
    "    for p in patches:\n",
    "        ax.add_patch(p)\n",
    "\n",
    "    plt.text(\n",
    "        1.45,\n",
    "        4.9,\n",
    "        \"Training\\nText,\\nDocuments,\\nImages,\\netc.\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    plt.text(3.6, 4.9, \"Feature\\nVectors\", ha=\"left\", va=\"center\", fontsize=14)\n",
    "\n",
    "    plt.text(\n",
    "        5.5, 3.5, \"Machine\\nLearning\\nAlgorithm\", ha=\"center\", va=\"center\", fontsize=14\n",
    "    )\n",
    "\n",
    "    plt.text(\n",
    "        1.05,\n",
    "        1.1,\n",
    "        \"New Text,\\nDocument,\\nImage,\\netc.\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    plt.text(3.3, 1.7, \"Feature\\nVector\", ha=\"left\", va=\"center\", fontsize=14)\n",
    "\n",
    "    plt.text(5.5, 1.1, \"Predictive\\nModel\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "    if supervised:\n",
    "        plt.text(1.45, 3.05, \"Labels\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "        plt.text(8.05, 1.1, \"Expected\\nLabel\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "        plt.text(\n",
    "            8.8, 5.8, \"Supervised Learning Model\", ha=\"right\", va=\"top\", fontsize=18\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        plt.text(\n",
    "            8.05,\n",
    "            1.1,\n",
    "            \"Likelihood\\nor Cluster ID\\nor Better\\nRepresentation\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        plt.text(\n",
    "            8.8, 5.8, \"Unsupervised Learning Model\", ha=\"right\", va=\"top\", fontsize=18\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_supervised_chart(annotate=False):\n",
    "    create_base(supervised=True)\n",
    "    if annotate:\n",
    "        fontdict = {\"color\": \"r\", \"weight\": \"bold\", \"size\": 14}\n",
    "        plt.text(\n",
    "            1.9,\n",
    "            4.55,\n",
    "            \"X = vec.fit_transform(input)\",\n",
    "            fontdict=fontdict,\n",
    "            rotation=20,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "        plt.text(\n",
    "            3.7,\n",
    "            3.2,\n",
    "            \"clf.fit(X, y)\",\n",
    "            fontdict=fontdict,\n",
    "            rotation=20,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "        plt.text(\n",
    "            1.7,\n",
    "            1.5,\n",
    "            \"X_new = vec.transform(input)\",\n",
    "            fontdict=fontdict,\n",
    "            rotation=20,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "        plt.text(\n",
    "            6.1,\n",
    "            1.5,\n",
    "            \"y_new = clf.predict(X_new)\",\n",
    "            fontdict=fontdict,\n",
    "            rotation=20,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unsupervised_chart():\n",
    "    create_base(supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b09bdb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    plot_supervised_chart(False)\n",
    "    plot_supervised_chart(True)\n",
    "    plot_unsupervised_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae3426",
   "metadata": {},
   "source": [
    "### Compare classifiers on the digits data\n",
    "\n",
    "<!--- plot_compare_classifiers -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a766f77",
   "metadata": {},
   "source": [
    "Compare the performance of a variety of classifiers on a test set for the\n",
    "digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, datasets, metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34018abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Model in [LinearSVC, GaussianNB, KNeighborsClassifier]:\n",
    "    clf = Model().fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"{Model.__name__}: {metrics.f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b890a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SVC loss\n",
    "for loss in [\"hinge\", \"squared_hinge\"]:\n",
    "    clf = LinearSVC(loss=loss).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\n",
    "        f\"LinearSVC(loss='{loss}'): {metrics.f1_score(y_test, y_pred, average='macro')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc55c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the number of neighbors\n",
    "for n_neighbors in range(1, 11):\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\n",
    "        f\"KNeighbors(n_neighbors={n_neighbors}): {metrics.f1_score(y_test, y_pred, average='macro')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4b85b",
   "metadata": {},
   "source": [
    "### The eigenfaces example: chaining PCA and SVMs\n",
    "\n",
    "<!--- plot_eigenfaces -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fcdfc",
   "metadata": {},
   "source": [
    "The goal of this example is to show how an unsupervised method and a\n",
    "supervised one can be chained for better prediction. It starts with a\n",
    "didactic but lengthy way of doing things, and finishes with the\n",
    "idiomatic approach to pipelining in scikit-learn.\n",
    "\n",
    "Here we'll take a look at a simple facial recognition example. Ideally,\n",
    "we would use a dataset consisting of a subset of the [Labeled Faces in\n",
    "the Wild](http://vis-www.cs.umass.edu/lfw) data that is available with\n",
    "{func}`sklearn.datasets.fetch_lfw_people`. However, this is a relatively large\n",
    "download (~200MB) so we will do the tutorial on a simpler, less rich dataset.\n",
    "Feel free to explore the LFW dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11222a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = datasets.fetch_olivetti_faces()\n",
    "faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a99f4",
   "metadata": {},
   "source": [
    "Let's visualize these faces to see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "# plot several images\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(faces.images[i], cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a9f9c",
   "metadata": {},
   "source": [
    "::: {note}\n",
    "\n",
    "Note that these faces have already been localized and scaled to a common size.\n",
    "This is an important preprocessing piece for facial recognition, and is\n",
    "a process that can require a large collection of training data. This can be\n",
    "done in scikit-learn, but the challenge is gathering a sufficient amount of\n",
    "training data for the algorithm to work. Fortunately, this piece is common\n",
    "enough that it has been done. One good resource is [OpenCV](https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)\n",
    " the _Open Computer Vision Library_.\n",
    "\n",
    ":::\n",
    "\n",
    "We'll perform a Support Vector classification of the images. We'll do a\n",
    "typical train-test split on the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107abe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbe9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    faces.data, faces.target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c33dc1",
   "metadata": {},
   "source": [
    "### Preprocessing: Principal Component Analysis\n",
    "\n",
    "1850 dimensions is a lot for SVM. We can use PCA to reduce these 1850\n",
    "features to a manageable size, while maintaining most of the information\n",
    "in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18139950",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=150, whiten=True)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80502b2",
   "metadata": {},
   "source": [
    "One interesting part of PCA is that it computes the \"mean\" face, which\n",
    "can be interesting to examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d21d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.mean_.reshape(faces.images[0].shape), cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42808551",
   "metadata": {},
   "source": [
    "The principal components measure deviations about this mean along\n",
    "orthogonal axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd09b55",
   "metadata": {},
   "source": [
    "It is also interesting to visualize these principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7627d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(pca.components_[i].reshape(faces.images[0].shape), cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5de7ba",
   "metadata": {},
   "source": [
    "The components (\"eigenfaces\") are ordered by their importance from\n",
    "top-left to bottom-right. We see that the first few components seem to\n",
    "primarily take care of lighting conditions; the remaining components\n",
    "pull out certain identifying features: the nose, eyes, eyebrows, etc.\n",
    "\n",
    "With this projection computed, we can now project our original training\n",
    "and test data onto the PCA basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2c1cf",
   "metadata": {},
   "source": [
    "These projected components correspond to factors in a linear combination\n",
    "of component images such that the combination approaches the original\n",
    "face.\n",
    "\n",
    "### Doing the Learning: Support Vector Machines\n",
    "\n",
    "Now we'll perform support-vector-machine classification on this reduced\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=5.0, gamma=0.001)\n",
    "clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebba14",
   "metadata": {},
   "source": [
    "Finally, we can evaluate how well this classification did. First, we\n",
    "might plot a few of the test-cases with the labels learned from the\n",
    "training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cef389",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(X_test[i].reshape(faces.images[0].shape), cmap=\"bone\")\n",
    "    y_pred = clf.predict(X_test_pca[i, np.newaxis])[0]\n",
    "    color = \"black\" if y_pred == y_test[i] else \"red\"\n",
    "    ax.set_title(y_pred, fontsize=\"small\", color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2e97a",
   "metadata": {},
   "source": [
    "The classifier is correct on an impressive number of images given the\n",
    "simplicity of its learning model! Using a linear classifier on 150\n",
    "features derived from the pixel-level data, the algorithm correctly\n",
    "identifies a large number of the people in the images.\n",
    "\n",
    "Again, we can quantify this effectiveness using one of several measures\n",
    "from {mod}`sklearn.metrics`. First we can do the classification\n",
    "report, which shows the precision, recall and other measures of the\n",
    "\"goodness\" of the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7308a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_pca)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac6007",
   "metadata": {},
   "source": [
    "Another interesting metric is the _confusion matrix_, which indicates\n",
    "how often any two items are mixed-up. The confusion matrix of a perfect\n",
    "classifier would only have nonzero entries on the diagonal, with zeros\n",
    "on the off-diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e242604",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "Above we used PCA as a pre-processing step before applying our support\n",
    "vector machine classifier. Plugging the output of one estimator directly\n",
    "into the input of a second estimator is a commonly used pattern; for\n",
    "this reason scikit-learn provides a `Pipeline` object which automates\n",
    "this process. The above problem can be re-expressed as a pipeline as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93295ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(\n",
    "    [\n",
    "        (\"pca\", decomposition.PCA(n_components=150, whiten=True)),\n",
    "        (\"svm\", svm.LinearSVC(C=1.0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf783f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe88e9",
   "metadata": {},
   "source": [
    "### A Note on Facial Recognition\n",
    "\n",
    "Here we have used PCA \"eigenfaces\" as a pre-processing step for facial\n",
    "recognition. The reason we chose this is because PCA is a\n",
    "broadly-applicable technique, which can be useful for a wide array of\n",
    "data types. Research in the field of facial recognition in particular,\n",
    "however, has shown that other more specific feature extraction methods\n",
    "are can be much more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706799b",
   "metadata": {},
   "source": [
    "### Example of linear and non-linear models\n",
    "\n",
    "<!--- plot_svm_non_linear -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90b8af",
   "metadata": {},
   "source": [
    "This is an example plot from the tutorial which accompanies an explanation\n",
    "of the support vector machine GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be27bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d29f7d",
   "metadata": {},
   "source": [
    "Data that is linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(rseed=42, n_samples=30):\n",
    "    \"Generate data according to a linear model\"\n",
    "    np.random.seed(rseed)\n",
    "\n",
    "    data = np.random.normal(0, 10, (n_samples, 2))\n",
    "    data[: n_samples // 2] -= 15\n",
    "    data[n_samples // 2 :] += 15\n",
    "\n",
    "    labels = np.ones(n_samples)\n",
    "    labels[: n_samples // 2] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = linear_model()\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot(111, xticks=[], yticks=[])\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bone\")\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=80,\n",
    "    edgecolors=\"k\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "delta = 1\n",
    "y_min, y_max = -50, 50\n",
    "x_min, x_max = -50, 50\n",
    "x = np.arange(x_min, x_max + delta, delta)\n",
    "y = np.arange(y_min, y_max + delta, delta)\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "Z = clf.decision_function(np.c_[X1.ravel(), X2.ravel()])\n",
    "Z = Z.reshape(X1.shape)\n",
    "ax.contour(\n",
    "    X1, X2, Z, [-1.0, 0.0, 1.0], colors=\"k\", linestyles=[\"dashed\", \"solid\", \"dashed\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bf229",
   "metadata": {},
   "source": [
    "Data with a non-linear separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_model(rseed=27446968, n_samples=30):\n",
    "    rng = np.random.default_rng(rseed)\n",
    "\n",
    "    radius = 40 * rng.random(n_samples)\n",
    "    far_pts = radius > 20\n",
    "    radius[far_pts] *= 1.2\n",
    "    radius[~far_pts] *= 1.1\n",
    "\n",
    "    theta = rng.random(n_samples) * np.pi * 2\n",
    "\n",
    "    data = np.empty((n_samples, 2))\n",
    "    data[:, 0] = radius * np.cos(theta)\n",
    "    data[:, 1] = radius * np.sin(theta)\n",
    "\n",
    "    labels = np.ones(n_samples)\n",
    "    labels[far_pts] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60be4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nonlinear_model()\n",
    "clf = svm.SVC(kernel=\"rbf\", gamma=0.001, coef0=0, degree=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot(1, 1, 1, xticks=[], yticks=[])\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bone\", zorder=2)\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=80,\n",
    "    edgecolors=\"k\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "delta = 1\n",
    "y_min, y_max = -50, 50\n",
    "x_min, x_max = -50, 50\n",
    "x = np.arange(x_min, x_max + delta, delta)\n",
    "y = np.arange(y_min, y_max + delta, delta)\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "Z = clf.decision_function(np.c_[X1.ravel(), X2.ravel()])\n",
    "Z = Z.reshape(X1.shape)\n",
    "ax.contour(\n",
    "    X1,\n",
    "    X2,\n",
    "    Z,\n",
    "    [-1.0, 0.0, 1.0],\n",
    "    colors=\"k\",\n",
    "    linestyles=[\"dashed\", \"solid\", \"dashed\"],\n",
    "    zorder=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5930577",
   "metadata": {},
   "source": [
    "### variance_linear_regr\n",
    "\n",
    "<!--- plot_variance_linear_regr -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d743d",
   "metadata": {},
   "source": [
    "Plot variance and regularization in linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceed2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider the situation where we have only 2 data point\n",
    "X = np.c_[0.5, 1].T\n",
    "y = [0.5, 1]\n",
    "X_test = np.c_[0, 2].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without noise, as linear regression fits the data perfectly\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "plt.plot(X, y, \"o\")\n",
    "plt.plot(X_test, regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real life situation, we have noise (e.g. measurement noise) in our data:\n",
    "rng = np.random.default_rng(27446968)\n",
    "for _ in range(6):\n",
    "    noisy_X = X + np.random.normal(loc=0, scale=0.1, size=X.shape)\n",
    "    plt.plot(noisy_X, y, \"o\")\n",
    "    regr.fit(noisy_X, y)\n",
    "    plt.plot(X_test, regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704acda",
   "metadata": {},
   "source": [
    "As we can see, our linear model captures and amplifies the noise in the\n",
    "data. It displays a lot of variance.\n",
    "\n",
    "We can use another linear estimator that uses regularization, the\n",
    "{class}`~sklearn.linear_model.Ridge` estimator. This estimator regularizes the\n",
    "coefficients by shrinking them to zero, under the assumption that very high\n",
    "correlations are often spurious. The alpha parameter controls the amount of\n",
    "shrinkage used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.Ridge(alpha=0.1)\n",
    "np.random.seed(0)\n",
    "for _ in range(6):\n",
    "    noisy_X = X + np.random.normal(loc=0, scale=0.1, size=X.shape)\n",
    "    plt.plot(noisy_X, y, \"o\")\n",
    "    regr.fit(noisy_X, y)\n",
    "    plt.plot(X_test, regr.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "name": "python"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
