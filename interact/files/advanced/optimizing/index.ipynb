{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6363668a",
   "metadata": {},
   "source": [
    "# Optimizing code\n",
    "\n",
    ":::{sidebar} Donald Knuth\n",
    "_“Premature optimization is the root of all evil”_\n",
    ":::\n",
    "\n",
    "**Author**: _Gaël Varoquaux_\n",
    "\n",
    "This chapter deals with strategies to make Python code go faster.\n",
    "\n",
    ":::{admonition} Prerequisites\n",
    "\n",
    "- [line_profiler](https://pypi.org/project/line-profiler/)\n",
    "  :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c128a35",
   "metadata": {},
   "source": [
    "## Optimization workflow\n",
    "\n",
    "1. Make it work: write the code in a simple **legible** ways.\n",
    "2. Make it work reliably: write automated test cases, make really sure\n",
    "   that your algorithm is right and that if you break it, the tests will\n",
    "   capture the breakage.\n",
    "3. Optimize the code by profiling simple use-cases to find the\n",
    "   bottlenecks and speeding up these bottleneck, finding a better\n",
    "   algorithm or implementation. Keep in mind that a trade off should be\n",
    "   found between profiling on a realistic example and the simplicity and\n",
    "   speed of execution of the code. For efficient work, it is best to work\n",
    "   with profiling runs lasting around 10s.\n",
    "\n",
    "## Profiling Python code\n",
    "\n",
    ":::{admonition} **No optimization without measuring!**\n",
    "\n",
    "- **Measure:** profiling, timing\n",
    "- You'll have surprises: the fastest code is not always what you think\n",
    "  :::\n",
    "\n",
    "### Timeit\n",
    "\n",
    "In Jupyter or IPython, use `timeit`\n",
    "(<https://docs.python.org/3/library/timeit.html>) to time elementary\n",
    "operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc61543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(1000)\n",
    "\n",
    "%timeit a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a77ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit a ** 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfe5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit a * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f56838",
   "metadata": {},
   "source": [
    "Use this to guide your choice between strategies.\n",
    "\n",
    ":::{note}\n",
    "For long running calls, using `%time` instead of `%timeit`; it is\n",
    "less precise but faster.\n",
    ":::\n",
    "\n",
    "### Profiler\n",
    "\n",
    "Useful when you have a large program to profile, for example the\n",
    "{download}`following file <demo.py>`:\n",
    "\n",
    "```{literalinclude} demo.py\n",
    "\n",
    "```\n",
    "\n",
    ":::{note}\n",
    "This is a combination of two unsupervised learning techniques, principal\n",
    "component analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) and\n",
    "independent component analysis\n",
    "([ICA](https://en.wikipedia.org/wiki/Independent_component_analysis)). PCA\n",
    "is a technique for dimensionality reduction, i.e. an algorithm to explain\n",
    "the observed variance in your data using less dimensions. ICA is a source\n",
    "separation technique, for example to unmix multiple signals that have been\n",
    "recorded through multiple sensors. Doing a PCA first and then an ICA can be\n",
    "useful if you have more sensors than signals. For more information see:\n",
    "[the FastICA example from scikits-learn](https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html).\n",
    ":::\n",
    "\n",
    "To run it, you also need to download the {download}`ica module <ica.py>`.\n",
    "In IPython we can time the script:\n",
    "\n",
    "```python\n",
    "In [1]: %run -t demo.py\n",
    "IPython CPU timings (estimated):\n",
    "    User  :    14.3929 s.\n",
    "    System:   0.256016 s.\n",
    "```\n",
    "\n",
    "and profile it:\n",
    "\n",
    "```python\n",
    "In [2]: %run -p demo.py\n",
    "        916 function calls in 14.551 CPU seconds\n",
    "Ordered by: internal time\n",
    "ncalls  tottime  percall  cumtime  percall filename:lineno (function)\n",
    "    1   14.457   14.457   14.479   14.479 decomp.py:849 (svd)\n",
    "    1    0.054    0.054    0.054    0.054 {method 'random_sample' of 'mtrand.RandomState' objects}\n",
    "    1    0.017    0.017    0.021    0.021 function_base.py:645 (asarray_chkfinite)\n",
    "    54    0.011    0.000    0.011    0.000 {numpy.core._dotblas.dot}\n",
    "    2    0.005    0.002    0.005    0.002 {method 'any' of 'numpy.ndarray' objects}\n",
    "    6    0.001    0.000    0.001    0.000 ica.py:195 (gprime)\n",
    "    6    0.001    0.000    0.001    0.000 ica.py:192 (g)\n",
    "    14    0.001    0.000    0.001    0.000 {numpy.linalg.lapack_lite.dsyevd}\n",
    "    19    0.001    0.000    0.001    0.000 twodim_base.py:204 (diag)\n",
    "    1    0.001    0.001    0.008    0.008 ica.py:69 (_ica_par)\n",
    "    1    0.001    0.001   14.551   14.551 {execfile}\n",
    "    107    0.000    0.000    0.001    0.000 defmatrix.py:239 (__array_finalize__)\n",
    "    7    0.000    0.000    0.004    0.001 ica.py:58 (_sym_decorrelation)\n",
    "    7    0.000    0.000    0.002    0.000 linalg.py:841 (eigh)\n",
    "    172    0.000    0.000    0.000    0.000 {isinstance}\n",
    "    1    0.000    0.000   14.551   14.551 demo.py:1 (<module>)\n",
    "    29    0.000    0.000    0.000    0.000 numeric.py:180 (asarray)\n",
    "    35    0.000    0.000    0.000    0.000 defmatrix.py:193 (__new__)\n",
    "    35    0.000    0.000    0.001    0.000 defmatrix.py:43 (asmatrix)\n",
    "    21    0.000    0.000    0.001    0.000 defmatrix.py:287 (__mul__)\n",
    "    41    0.000    0.000    0.000    0.000 {numpy.core.multiarray.zeros}\n",
    "    28    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
    "    1    0.000    0.000    0.008    0.008 ica.py:97 (fastica)\n",
    "    ...\n",
    "```\n",
    "\n",
    "Clearly the `svd` (in `decomp.py`) is what takes most of our time, a.k.a. the\n",
    "bottleneck. We have to find a way to make this step go faster, or to avoid this\n",
    "step (algorithmic optimization). Spending time on the rest of the code is\n",
    "useless.\n",
    "\n",
    ":::{admonition} **Profiling outside of IPython, running \\`\\`cProfile\\`\\`**\n",
    "Similar profiling can be done outside of IPython, simply calling the\n",
    "built-in [Python profilers](https://docs.python.org/3/library/profile.html) `cProfile` and\n",
    "`profile`.\n",
    "\n",
    "```console\n",
    "$  python -m cProfile -o demo.prof demo.py\n",
    "```\n",
    "\n",
    "Using the `-o` switch will output the profiler results to the file\n",
    "`demo.prof` to view with an external tool. This can be useful if\n",
    "you wish to process the profiler output with a visualization tool.\n",
    ":::\n",
    "\n",
    "### Line-profiler\n",
    "\n",
    "The profiler tells us which function takes most of the time, but not\n",
    "where it is called.\n",
    "\n",
    "For this, we use the\n",
    "[line_profiler](https://pypi.org/project/line-profiler/): in the\n",
    "source file, we decorate a few functions that we want to inspect with\n",
    "`@profile` (no need to import it)\n",
    "\n",
    "```python\n",
    "@profile\n",
    "def test():\n",
    "    rng = np.random.default_rng()\n",
    "    data = rng.random((5000, 100))\n",
    "    u, s, v = linalg.svd(data)\n",
    "    pca = u[:, :10] @ data\n",
    "    results = fastica(pca.T, whiten=False)\n",
    "```\n",
    "\n",
    "Then we run the script using the [kernprof](https://pypi.org/project/line-profiler/) command, with switches `-l, --line-by-line` and `-v, --view` to use the line-by-line profiler and view the results in addition to saving them:\n",
    "\n",
    "```console\n",
    "$ kernprof -l -v demo.py\n",
    "\n",
    "Wrote profile results to demo.py.lprof\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 1.27874 s\n",
    "File: demo.py\n",
    "Function: test at line 9\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     9                                           @profile\n",
    "    10                                           def test():\n",
    "    11         1         69.0     69.0      0.0      rng = np.random.default_rng()\n",
    "    12         1       2453.0   2453.0      0.2      data = rng.random((5000, 100))\n",
    "    13         1    1274715.0 1274715.0     99.7      u, s, v = sp.linalg.svd(data)\n",
    "    14         1        413.0    413.0      0.0      pca = u[:, :10].T @ data\n",
    "    15         1       1094.0   1094.0      0.1      results = fastica(pca.T, whiten=False)\n",
    "```\n",
    "\n",
    "**The SVD is taking all the time.** We need to optimise this line.\n",
    "\n",
    "## Making code go faster\n",
    "\n",
    "Once we have identified the bottlenecks, we need to make the\n",
    "corresponding code go faster.\n",
    "\n",
    "### Algorithmic optimization\n",
    "\n",
    "The first thing to look for is algorithmic optimization: are there ways\n",
    "to compute less, or better?\n",
    "\n",
    "For a high-level view of the problem, a good understanding of the maths\n",
    "behind the algorithm helps. However, it is not uncommon to find simple\n",
    "changes, like **moving computation or memory allocation outside a for\n",
    "loop**, that bring in big gains.\n",
    "\n",
    "#### Example of the SVD\n",
    "\n",
    "In both examples above, the SVD -\n",
    "[Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "\\- is what\n",
    "takes most of the time. Indeed, the computational cost of this algorithm is\n",
    "roughly $n^3$ in the size of the input matrix.\n",
    "\n",
    "However, in both of these example, we are not using all the output of\n",
    "the SVD, but only the first few rows of its first return argument. If\n",
    "we use the `svd` implementation of SciPy, we can ask for an incomplete\n",
    "version of the SVD. Note that implementations of linear algebra in\n",
    "SciPy are richer then those in NumPy and should be preferred.\n",
    "\n",
    "```python\n",
    "In [3]: %timeit np.linalg.svd(data)\n",
    "1 loops, best of 3: 14.5 s per loop\n",
    "\n",
    "In [4]: import scipy as sp\n",
    "\n",
    "In [5]: %timeit sp.linalg.svd(data)\n",
    "1 loops, best of 3: 14.2 s per loop\n",
    "\n",
    "In [6]: %timeit sp.linalg.svd(data, full_matrices=False)\n",
    "1 loops, best of 3: 295 ms per loop\n",
    "\n",
    "In [7]: %timeit np.linalg.svd(data, full_matrices=False)\n",
    "1 loops, best of 3: 293 ms per loop\n",
    "```\n",
    "\n",
    "We can then use this insight to {download}`optimize the previous code <demo_opt.py>`:\n",
    "\n",
    "```{literalinclude} demo_opt.py\n",
    ":pyobject: test\n",
    "```\n",
    "\n",
    "```python\n",
    "In [1]: import demo\n",
    "\n",
    "In [2]: %timeit demo.\n",
    "demo.fastica   demo.np        demo.prof.pdf  demo.py        demo.pyc\n",
    "demo.linalg    demo.prof      demo.prof.png  demo.py.lprof  demo.test\n",
    "\n",
    "In [2]: %timeit demo.test()\n",
    "ica.py:65: RuntimeWarning: invalid value encountered in sqrt\n",
    "  W = (u * np.diag(1.0/np.sqrt(s)) * u.T) * W  # W = (W * W.T) ^{-1/2} * W\n",
    "1 loops, best of 3: 17.5 s per loop\n",
    "\n",
    "In [3]: import demo_opt\n",
    "\n",
    "In [4]: %timeit demo_opt.test()\n",
    "1 loops, best of 3: 208 ms per loop\n",
    "```\n",
    "\n",
    "Real incomplete SVDs, e.g. computing only the first 10 eigenvectors, can\n",
    "be computed with ARPACK, available in `scipy.sparse.linalg.eigsh`.\n",
    "\n",
    ":::{admonition} Computational linear algebra\n",
    "For certain algorithms, many of the bottlenecks will be linear\n",
    "algebra computations. In this case, using the right function to solve\n",
    "the right problem is key. For instance, an eigenvalue problem with a\n",
    "symmetric matrix is easier to solve than with a general matrix. Also,\n",
    "most often, you can avoid inverting a matrix and use a less costly\n",
    "(and more numerically stable) operation.\n",
    "\n",
    "Know your computational linear algebra. When in doubt, explore\n",
    "`scipy.linalg`, and use `%timeit` to try out different alternatives\n",
    "on your data.\n",
    ":::\n",
    "\n",
    "## Writing faster numerical code\n",
    "\n",
    "A complete discussion on advanced use of NumPy is found in chapter\n",
    "{ref}`advanced-numpy`, or in the article [The NumPy array: a structure for\n",
    "efficient numerical computation](https://hal.inria.fr/inria-00564007/en).\n",
    "by van der Walt _et al._ Here we discuss only some commonly encountered tricks\n",
    "to make code faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec053e50",
   "metadata": {},
   "source": [
    "### Vectorizing for loops\n",
    "\n",
    "Find tricks to avoid for loops using NumPy arrays. For this, masks and\n",
    "indices arrays can be useful.\n",
    "\n",
    "### Broadcasting\n",
    "\n",
    "Use {ref}`broadcasting <broadcasting>` to do operations on arrays as\n",
    "small as possible before combining them.\n",
    "\n",
    "<!---\n",
    "XXX: complement broadcasting in the NumPy chapter with the example of\n",
    "the 3D grid\n",
    "-->\n",
    "\n",
    "### In place operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56674386",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(10_000_000)\n",
    "\n",
    "%timeit global a ; a = 0*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42568bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit global a ; a *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35accf",
   "metadata": {},
   "source": [
    "**note**: we need `global a` in the `timeit` so that it works as expected, as\n",
    "otherwise it is assigning to `a`, and thus considers it as a local variable.\n",
    "\n",
    "### Be easy on the memory: use views, and not copies\n",
    "\n",
    "Copying big arrays is as costly as making simple numerical operations\n",
    "on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(10_000_000)\n",
    "\n",
    "%timeit a.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b0c73",
   "metadata": {},
   "source": [
    "### Beware of cache effects\n",
    "\n",
    "Memory access is cheaper when it is grouped: accessing a big array in a\n",
    "continuous way is much faster than random access. This implies amongst\n",
    "other things that **smaller strides are faster** (see\n",
    "{ref}`cache-effects`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b630c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.zeros((5000, 5000), order='C')\n",
    "\n",
    "# Row elements are far apart in memory, for C ordering.\n",
    "%timeit np.median(c, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abada8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column elements are contiguous in memory, for C ordering.\n",
    "%timeit np.median(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c205c",
   "metadata": {},
   "source": [
    "This is the reason why Fortran ordering or C ordering may make a big\n",
    "difference on speed of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "a = rng.random((20, 2**18))\n",
    "\n",
    "b = rng.random((20, 2**18))\n",
    "\n",
    "%timeit b @ a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.ascontiguousarray(a.T)\n",
    "\n",
    "%timeit b @ c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0075f12",
   "metadata": {},
   "source": [
    "Note that copying the data to work around this effect may not be worth it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit c = np.ascontiguousarray(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e51369",
   "metadata": {},
   "source": [
    "Using [numexpr](https://github.com/pydata/numexpr) can be useful to\n",
    "automatically optimize code for such effects.\n",
    "\n",
    "### Use compiled code\n",
    "\n",
    "The last resort, once you are sure that all the high-level optimizations have\n",
    "been explored, is to transfer the hot spots, i.e. the few lines or functions\n",
    "in which most of the time is spent, to compiled code. For compiled code, the\n",
    "preferred option is to use [Cython](https://www.cython.org): it is easy to\n",
    "transform exiting Python code in compiled code, and with a good use of the\n",
    "[NumPy support](https://docs.cython.org/en/latest/src/tutorial/numpy.html)\n",
    "yields efficient code on NumPy arrays, for instance by unrolling loops.\n",
    "\n",
    ":::{warning}\n",
    "For all the above: profile and time your choices. Don't base your\n",
    "optimization on theoretical considerations.\n",
    ":::\n",
    "\n",
    "## Additional Links\n",
    "\n",
    "- If you need to profile memory usage, you could try the\n",
    "  [memory_profiler](https://pypi.org/project/memory-profiler)\n",
    "- If you need to profile down into C extensions, you could try using\n",
    "  [gperftools](https://github.com/gperftools/gperftools) from Python with\n",
    "  [yep](https://pypi.org/project/yep).\n",
    "- If you would like to track performance of your code across time, i.e. as you\n",
    "  make new commits to your repository, you could try:\n",
    "  [asv](https://asv.readthedocs.io/en/stable/)\n",
    "- If you need some interactive visualization why not try\n",
    "  [RunSnakeRun](https://www.vrplumber.com/programming/runsnakerun/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
