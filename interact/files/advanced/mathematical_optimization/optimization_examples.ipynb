{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972f7fea",
   "metadata": {},
   "source": [
    "(optimization-examples)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe77ed6",
   "metadata": {},
   "source": [
    "# Examples for mathematical optimization page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e717f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machinery to store outputs for later use.\n",
    "# This is for rendering in the Jupyter Book version of these pages.\n",
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd005",
   "metadata": {},
   "source": [
    "(convex-function-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df506a",
   "metadata": {},
   "source": [
    "## Convex function\n",
    "\n",
    "<!--- plot_convex -->\n",
    "\n",
    "A figure showing the definition of a convex function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ccde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "# A convex function\n",
    "plt.plot(x, x**2, linewidth=2)\n",
    "plt.text(-0.7, -(0.6**2), \"$f$\", size=20)\n",
    "\n",
    "# The tangent in one point\n",
    "plt.plot(x, 2 * x - 1)\n",
    "plt.plot(1, 1, \"k+\")\n",
    "plt.text(0.3, -0.75, \"Tangent to $f$\", size=15)\n",
    "plt.text(1, 1 - 0.5, \"C\", size=15)\n",
    "\n",
    "# Convexity as barycenter\n",
    "plt.plot([0.35, 1.85], [0.35**2, 1.85**2])\n",
    "plt.plot([0.35, 1.85], [0.35**2, 1.85**2], \"k+\")\n",
    "plt.text(0.35 - 0.2, 0.35**2 + 0.1, \"A\", size=15)\n",
    "plt.text(1.85 - 0.2, 1.85**2, \"B\", size=15)\n",
    "\n",
    "plt.ylim(ymin=-1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"convex_func\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28222a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convexity as barycenter\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, x**2 + np.exp(-5 * (x - 0.5) ** 2), linewidth=2)\n",
    "plt.text(-0.7, -(0.6**2), \"$f$\", size=20)\n",
    "\n",
    "plt.ylim(ymin=-1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"non_convex_func\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f6d34",
   "metadata": {},
   "source": [
    "(smooth-function-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71b354",
   "metadata": {},
   "source": [
    "## Smooth and non-smooth functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ffebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "x = np.linspace(-1.5, 1.5, 101)\n",
    "\n",
    "# A smooth function\n",
    "\n",
    "plt.plot(x, np.sqrt(0.2 + x**2), linewidth=2)\n",
    "plt.text(-1, 0, \"$f$\", size=20)\n",
    "\n",
    "plt.ylim(ymin=-0.2)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"smooth_func\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A non-smooth function\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, np.abs(x), linewidth=2)\n",
    "plt.text(-1, 0, \"$f$\", size=20)\n",
    "\n",
    "plt.ylim(ymin=-0.2)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"non_smooth_func\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cad292",
   "metadata": {},
   "source": [
    "(noisy-non-noisy-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc53156",
   "metadata": {},
   "source": [
    "## Noisy and non-noisy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)\n",
    "\n",
    "x = np.linspace(-5, 5, 101)\n",
    "x_ = np.linspace(-5, 5, 31)\n",
    "\n",
    "# A smooth function\n",
    "def f(x):\n",
    "    return -np.exp(-(x**2))\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_, f(x_) + 0.2 * rng.normal(size=31), linewidth=2)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "\n",
    "plt.ylim(ymin=-1.3)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"noisy_non_noisy\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f08c28",
   "metadata": {},
   "source": [
    "(constraints-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6199d9",
   "metadata": {},
   "source": [
    "## Optimizing with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-2.9:5.8:0.05, -2.5:5:0.05]  # type: ignore[misc]\n",
    "x = x.T\n",
    "y = y.T\n",
    "\n",
    "def make_constraint_fig():\n",
    "    fig = plt.figure(figsize=(3, 2.5))\n",
    "    contours = plt.contour(\n",
    "        np.sqrt((x - 3) ** 2 + (y - 2) ** 2),\n",
    "        extent=[-3, 6, -2.5, 5],\n",
    "        cmap=\"gnuplot\",\n",
    "    )\n",
    "    plt.clabel(contours, inline=1, fmt=\"%1.1f\", fontsize=14)\n",
    "    plt.plot(\n",
    "        [-1.5, -1.5, 1.5, 1.5, -1.5], [-1.5, 1.5, 1.5, -1.5, -1.5], \"k\", linewidth=2\n",
    "    )\n",
    "    plt.fill_between([-1.5, 1.5], [-1.5, -1.5], [1.5, 1.5], color=\".8\")\n",
    "    plt.axvline(0, color=\"k\")\n",
    "    plt.axhline(0, color=\"k\")\n",
    "\n",
    "    plt.text(-0.9, 4.4, \"$x_2$\", size=20)\n",
    "    plt.text(5.6, -0.6, \"$x_1$\", size=20)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis(\"off\")\n",
    "    return fig\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(\"constraints_no_path\", make_constraint_fig(), display=False)\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = []\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3) ** 2 + (x[1] - 2) ** 2)\n",
    "\n",
    "\n",
    "# We don't use the gradient, as with the gradient, L-BFGS is too fast,\n",
    "# and finds the optimum without showing us a pretty path\n",
    "def f_prime(x):\n",
    "    r = np.sqrt((x[0] - 3) ** 2 + (x[0] - 2) ** 2)\n",
    "    return np.array(((x[0] - 3) / r, (x[0] - 2) / r))\n",
    "\n",
    "\n",
    "sp.optimize.minimize(\n",
    "    f, np.array([0, 0]), method=\"L-BFGS-B\", bounds=((-1.5, 1.5), (-1.5, 1.5))\n",
    ")\n",
    "accumulated = np.array(accumulator)\n",
    "\n",
    "fig = make_constraint_fig()\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1]);\n",
    "\n",
    "glue(\"constraints_path\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096e8b0",
   "metadata": {},
   "source": [
    "(brents-method-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfc7fb",
   "metadata": {},
   "source": [
    "## Brent's method for convex and not-convex functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff44a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 3, 100)\n",
    "x_0 = np.exp(-1)\n",
    "\n",
    "def func(x, epsilon):\n",
    "    return (x - x_0)**2 + epsilon * np.exp(-5 * (x - .5 - x_0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in (0, 1):\n",
    "\n",
    "    f = lambda x : func(x, epsilon)\n",
    "\n",
    "    plt.figure(figsize=(3, 2.5))\n",
    "    plt.axes((0, 0, 1, 1))\n",
    "\n",
    "    # A convex function\n",
    "    plt.plot(x, f(x), linewidth=2)\n",
    "\n",
    "    # Apply Brent method. To have access to the iteration, do this in an\n",
    "    # artificial way: allow the algorithm to iter only once\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    for iter in range(30):\n",
    "        result = sp.optimize.minimize_scalar(\n",
    "            f,\n",
    "            bracket=(-5, 2.9, 4.5),\n",
    "            method=\"Brent\",\n",
    "            options={\"maxiter\": iter},\n",
    "            tol=np.finfo(1.0).eps,\n",
    "        )\n",
    "        if result.success:\n",
    "            print(\"Converged at \", iter)\n",
    "            break\n",
    "\n",
    "        this_x = result.x\n",
    "        all_x.append(this_x)\n",
    "        all_y.append(f(this_x))\n",
    "        if iter < 6:\n",
    "            plt.text(\n",
    "                this_x - 0.05 * np.sign(this_x) - 0.05,\n",
    "                f(this_x) + 1.2 * (0.3 - iter % 2),\n",
    "                str(iter + 1),\n",
    "                size=12,\n",
    "            )\n",
    "\n",
    "    plt.plot(all_x[:10], all_y[:10], \"k+\", markersize=12, markeredgewidth=2)\n",
    "\n",
    "    plt.plot(all_x[-1], all_y[-1], \"rx\", markersize=12)\n",
    "    plt.axis(\"off\")\n",
    "    plt.ylim(ymin=-1, ymax=8)\n",
    "\n",
    "    # Store figure for use in page.\n",
    "    glue(f\"brent_epsilon_{epsilon}_func\", plt.gcf(), display=False)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.semilogy(np.abs(all_y - all_y[-1]), linewidth=2)\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Store figure for use in page.\n",
    "    glue(f\"brent_epsilon_{epsilon}_err\", plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97fbe4",
   "metadata": {},
   "source": [
    "(gradient-descent-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68aa8b7",
   "metadata": {},
   "source": [
    "## Gradient descent examples\n",
    "\n",
    "An example demoing gradient descent by creating figures that trace the\n",
    "evolution of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparatory work for loading helper code.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"helper\"))\n",
    "\n",
    "from cost_functions import (\n",
    "    mk_quad,\n",
    "    mk_gauss,\n",
    "    rosenbrock,\n",
    "    rosenbrock_prime,\n",
    "    rosenbrock_hessian,\n",
    "    LoggingFunction,\n",
    "    CountingFunction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = -1, 2\n",
    "y_min, y_max = 2.25 / 3 * x_min - 0.2, 2.25 / 3 * x_max - 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a8fc8",
   "metadata": {},
   "source": [
    "A formatter to print values on contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_fmt(value):\n",
    "    if value > 1:\n",
    "        if np.abs(int(value) - value) < 0.1:\n",
    "            out = f\"$10^{{{int(value):d}}}$\"\n",
    "        else:\n",
    "            out = f\"$10^{{{value:.1f}}}$\"\n",
    "    else:\n",
    "        value = np.exp(value - 0.01)\n",
    "        if value > 0.1:\n",
    "            out = f\"{value:1.1f}\"\n",
    "        elif value > 0.01:\n",
    "            out = f\"{value:.2f}\"\n",
    "        else:\n",
    "            out = f\"{value:.2e}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64692949",
   "metadata": {},
   "source": [
    "A gradient descent algorithm.\n",
    "\n",
    "Do not use for production work: its a toy, use scipy's `optimize.fmin_cg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61769c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f, f_prime, hessian=None, adaptative=False):\n",
    "    x_i, y_i = x0\n",
    "    all_x_i = []\n",
    "    all_y_i = []\n",
    "    all_f_i = []\n",
    "\n",
    "    for i in range(1, 100):\n",
    "        all_x_i.append(x_i)\n",
    "        all_y_i.append(y_i)\n",
    "        all_f_i.append(f([x_i, y_i]))\n",
    "        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n",
    "        if adaptative:\n",
    "            # Compute a step size using a line_search to satisfy the Wolf\n",
    "            # conditions\n",
    "            step = sp.optimize.line_search(\n",
    "                f,\n",
    "                f_prime,\n",
    "                np.r_[x_i, y_i],\n",
    "                -np.r_[dx_i, dy_i],\n",
    "                np.r_[dx_i, dy_i],\n",
    "                c2=0.05,\n",
    "            )\n",
    "            step = step[0]\n",
    "            if step is None:\n",
    "                step = 0\n",
    "        else:\n",
    "            step = 1\n",
    "        x_i += -step * dx_i\n",
    "        y_i += -step * dy_i\n",
    "        if np.abs(all_f_i[-1]) < 1e-16:\n",
    "            break\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def gradient_descent_adaptative(x0, f, f_prime, hessian=None):\n",
    "    return gradient_descent(x0, f, f_prime, adaptative=True)\n",
    "\n",
    "\n",
    "def conjugate_gradient(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, jac=f_prime, method=\"CG\", callback=store, options={\"gtol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def newton_cg(x0, f, f_prime, hessian):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f,\n",
    "        x0,\n",
    "        method=\"Newton-CG\",\n",
    "        jac=f_prime,\n",
    "        hess=hessian,\n",
    "        callback=store,\n",
    "        options={\"xtol\": 1e-12},\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def bfgs(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"BFGS\", jac=f_prime, callback=store, options={\"gtol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def powell(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"Powell\", callback=store, options={\"ftol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def nelder_mead(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"Nelder-Mead\", callback=store, options={\"ftol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589822e",
   "metadata": {},
   "source": [
    "Run different optimizers on these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210830dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {}\n",
    "\n",
    "for name, (f, f_prime, hessian), optimizer in (\n",
    "    ('q_07_gd', mk_quad(0.7), gradient_descent),\n",
    "    ('q_07_gda', mk_quad(0.7), gradient_descent_adaptative),\n",
    "    ('q_002_gd', mk_quad(0.02), gradient_descent),\n",
    "    ('q_002_gda', mk_quad(0.02), gradient_descent_adaptative),\n",
    "    ('g_002_gda', mk_gauss(0.02), gradient_descent_adaptative),\n",
    "    (\n",
    "        'rb_gda',\n",
    "        (rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "        gradient_descent_adaptative,\n",
    "    ),\n",
    "    ('g_002_cg', mk_gauss(0.02), conjugate_gradient),\n",
    "    (\n",
    "        'rb_cg',\n",
    "        (rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "        conjugate_gradient,\n",
    "    ),\n",
    "    ('q_002_ncg', mk_quad(0.02), newton_cg),\n",
    "    ('g_002_ncg', mk_gauss(0.02), newton_cg),\n",
    "    (\n",
    "        'rb_ncg',\n",
    "        (rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "        newton_cg,\n",
    "    ),\n",
    "    ('q_002_bgfs', mk_quad(0.02), bfgs),\n",
    "    ('g_002_bgfs', mk_gauss(0.02), bfgs),\n",
    "    ('rb_bgfs', (rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "    ('q_002_pow', mk_quad(0.02), powell),\n",
    "    ('g_002_pow', mk_gauss(0.02), powell),\n",
    "    ('rb_pow', (rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "    ('g_002_nm', mk_gauss(0.02), nelder_mead),\n",
    "    ('rb_nm', (rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    # Plot the contour plot\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    plt.figure(figsize=(3, 2.5))\n",
    "    plt.axes([0, 0, 1, 1])\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    # Store figure for use in page.\n",
    "    glue(f'gradient_descent_{name}_func', plt.gcf(), display=False)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30),\n",
    "                 linewidth=2,\n",
    "                 label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Store figure for use in page.\n",
    "    glue(f'gradient_descent_{name}_err', plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc2ee0",
   "metadata": {},
   "source": [
    "(compare-optimizers-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29ac00",
   "metadata": {},
   "source": [
    "## Plotting the comparison of optimizers\n",
    "\n",
    "Plots the results from the comparison of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f186985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('helper/compare_optimizers_py3.pkl', 'rb') as fobj:\n",
    "    results = pickle.load(fobj)\n",
    "\n",
    "n_methods = len(list(results.values())[0][\"Rosenbrock  \"])\n",
    "n_dims = len(results)\n",
    "\n",
    "symbols = \"o>*Ds\"\n",
    "\n",
    "plt.figure(1, figsize=(10, 4))\n",
    "plt.clf()\n",
    "\n",
    "nipy_spectral = plt.colormaps[\"nipy_spectral\"]\n",
    "colors = nipy_spectral(np.linspace(0, 1, n_dims))[:, :3]\n",
    "\n",
    "method_names = list(list(results.values())[0][\"Rosenbrock  \"].keys())\n",
    "method_names.sort(key=lambda x: x[::-1], reverse=True)\n",
    "\n",
    "for n_dim_index, ((n_dim, n_dim_bench), color) in enumerate(\n",
    "    zip(sorted(results.items()), colors, strict=True)\n",
    "):\n",
    "    for (cost_name, cost_bench), symbol in zip(\n",
    "        sorted(n_dim_bench.items()), symbols, strict=True\n",
    "    ):\n",
    "        for (\n",
    "            method_index,\n",
    "            method_name,\n",
    "        ) in enumerate(method_names):\n",
    "            this_bench = cost_bench[method_name]\n",
    "            bench = np.mean(this_bench)\n",
    "            plt.semilogy([method_index + 0.1 * n_dim_index],\n",
    "                         [bench],\n",
    "                         marker=symbol,\n",
    "                         color=color)\n",
    "\n",
    "# Create a legend for the problem type\n",
    "for cost_name, symbol in zip(sorted(n_dim_bench.keys()), symbols, strict=True):\n",
    "    plt.semilogy([-10], [0], symbol, color=\".5\", label=cost_name)\n",
    "\n",
    "plt.xticks(np.arange(n_methods), method_names, size=11)\n",
    "plt.xlim(-0.2, n_methods - 0.5)\n",
    "plt.legend(loc=\"best\", numpoints=1, handletextpad=0, prop={\"size\": 12}, frameon=False)\n",
    "plt.ylabel(\"# function calls (a.u.)\")\n",
    "\n",
    "# Create a second legend for the problem dimensionality\n",
    "plt.twinx()\n",
    "\n",
    "for n_dim, color in zip(sorted(results.keys()), colors, strict=True):\n",
    "    plt.plot([-10], [0], \"o\", color=color, label=f\"# dim: {n_dim}\")\n",
    "\n",
    "plt.legend(\n",
    "    loc=(0.47, 0.07),\n",
    "    numpoints=1,\n",
    "    handletextpad=0,\n",
    "    prop={\"size\": 12},\n",
    "    frameon=False,\n",
    "    ncol=2,\n",
    ")\n",
    "plt.xlim(-0.2, n_methods - 0.5)\n",
    "\n",
    "plt.xticks(np.arange(n_methods), method_names)\n",
    "plt.yticks(())\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(f'compare_optimizers', plt.gcf(), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1857dfd",
   "metadata": {},
   "source": [
    "(constraints-non-bounds-eg)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f17d7d",
   "metadata": {},
   "source": [
    "## Optimization with constraints, SLSQP and COBYLA\n",
    "\n",
    "An example showing how to do optimization with general constraints using SLSQP\n",
    "and COBYLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddde725",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-2.03:4.2:0.04, -1.6:3.2:0.04]\n",
    "x = x.T\n",
    "y = y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2.5))\n",
    "plt.axes((0, 0, 1, 1))\n",
    "\n",
    "contours = plt.contour(\n",
    "    np.sqrt((x - 3) ** 2 + (y - 2) ** 2),\n",
    "    extent=[-2.03, 4.2, -1.6, 3.2],\n",
    "    cmap=\"gnuplot\",\n",
    ")\n",
    "plt.clabel(contours, inline=1, fmt=\"%1.1f\", fontsize=14)\n",
    "plt.plot([-1.5, 0, 1.5, 0, -1.5], [0, 1.5, 0, -1.5, 0], \"k\", linewidth=2)\n",
    "plt.fill_between([-1.5, 0, 1.5], [0, -1.5, 0], [0, 1.5, 0], color=\".8\")\n",
    "plt.axvline(0, color=\"k\")\n",
    "plt.axhline(0, color=\"k\")\n",
    "\n",
    "plt.text(-0.9, 2.8, \"$x_2$\", size=20)\n",
    "plt.text(3.6, -0.6, \"$x_1$\", size=20)\n",
    "plt.axis(\"tight\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = []\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3) ** 2 + (x[1] - 2) ** 2)\n",
    "\n",
    "\n",
    "def constraint(x):\n",
    "    return np.atleast_1d(1.5 - np.sum(np.abs(x)))\n",
    "\n",
    "\n",
    "sp.optimize.minimize(\n",
    "    f, np.array([0, 0]), method=\"SLSQP\", constraints={\"fun\": constraint, \"type\": \"ineq\"}\n",
    ")\n",
    "\n",
    "accumulated = np.array(accumulator)\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1])\n",
    "\n",
    "# Store figure for use in page.\n",
    "glue(f'constraints_non_bounds', plt.gcf(), display=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-language_info",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "name": "python"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
