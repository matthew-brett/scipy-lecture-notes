{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1214474",
   "metadata": {},
   "source": [
    "# Mathematical optimization: finding minima of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3226dd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "import collections\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"helper\"))\n",
    "from cost_functions import (\n",
    "    mk_quad,\n",
    "    mk_gauss,\n",
    "    rosenbrock,\n",
    "    rosenbrock_prime,\n",
    "    rosenbrock_hessian,\n",
    "    LoggingFunction,\n",
    "    CountingFunction,\n",
    ")\n",
    "\n",
    "# A custom function for plotting.\n",
    "def get_subplot_n(index):\n",
    "    row = index+1\n",
    "    if row == 1:\n",
    "        subplot_n0 = 1\n",
    "        subplot_n1 = 2\n",
    "        subplot_n2 = 3\n",
    "    elif row == 2:\n",
    "        subplot_n0 = 4\n",
    "        subplot_n1 = 5\n",
    "        subplot_n2 = 6\n",
    "    elif row == 3:\n",
    "        subplot_n0 = 7\n",
    "        subplot_n1 = 8\n",
    "        subplot_n2 = 9\n",
    "    elif row == 4:\n",
    "        subplot_n0 = 10\n",
    "        subplot_n1 = 11\n",
    "        subplot_n2 = 12\n",
    "    return subplot_n0, subplot_n1, subplot_n2\n",
    "\n",
    "# Functions for gradient descent\n",
    "\n",
    "###############################################################################\n",
    "# A formatter to print values on contours\n",
    "def super_fmt(value):\n",
    "    if value > 1:\n",
    "        if np.abs(int(value) - value) < 0.1:\n",
    "            out = f\"$10^{{{int(value):d}}}$\"\n",
    "        else:\n",
    "            out = f\"$10^{{{value:.1f}}}$\"\n",
    "    else:\n",
    "        value = np.exp(value - 0.01)\n",
    "        if value > 0.1:\n",
    "            out = f\"{value:1.1f}\"\n",
    "        elif value > 0.01:\n",
    "            out = f\"{value:.2f}\"\n",
    "        else:\n",
    "            out = f\"{value:.2e}\"\n",
    "    return out\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# A gradient descent algorithm\n",
    "# do not use: its a toy, use scipy's optimize.fmin_cg\n",
    "\n",
    "def gradient_descent(x0, f, f_prime, hessian=None, adaptative=False):\n",
    "    x_i, y_i = x0\n",
    "    all_x_i = []\n",
    "    all_y_i = []\n",
    "    all_f_i = []\n",
    "\n",
    "    for i in range(1, 100):\n",
    "        all_x_i.append(x_i)\n",
    "        all_y_i.append(y_i)\n",
    "        all_f_i.append(f([x_i, y_i]))\n",
    "        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n",
    "        if adaptative:\n",
    "            # Compute a step size using a line_search to satisfy the Wolf\n",
    "            # conditions\n",
    "            step = sp.optimize.line_search(\n",
    "                f,\n",
    "                f_prime,\n",
    "                np.r_[x_i, y_i],\n",
    "                -np.r_[dx_i, dy_i],\n",
    "                np.r_[dx_i, dy_i],\n",
    "                c2=0.05,\n",
    "            )\n",
    "            step = step[0]\n",
    "            if step is None:\n",
    "                step = 0\n",
    "        else:\n",
    "            step = 1\n",
    "        x_i += -step * dx_i\n",
    "        y_i += -step * dy_i\n",
    "        if np.abs(all_f_i[-1]) < 1e-16:\n",
    "            break\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "def gradient_descent_adaptative(x0, f, f_prime, hessian=None):\n",
    "    return gradient_descent(x0, f, f_prime, adaptative=True)\n",
    "\n",
    "def conjugate_gradient(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, jac=f_prime, method=\"CG\", callback=store, options={\"gtol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def newton_cg(x0, f, f_prime, hessian):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f,\n",
    "        x0,\n",
    "        method=\"Newton-CG\",\n",
    "        jac=f_prime,\n",
    "        hess=hessian,\n",
    "        callback=store,\n",
    "        options={\"xtol\": 1e-12},\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def bfgs(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"BFGS\", jac=f_prime, callback=store, options={\"gtol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def powell(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"Powell\", callback=store, options={\"ftol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i\n",
    "\n",
    "\n",
    "def nelder_mead(x0, f, f_prime, hessian=None):\n",
    "    all_x_i = [x0[0]]\n",
    "    all_y_i = [x0[1]]\n",
    "    all_f_i = [f(x0)]\n",
    "\n",
    "    def store(X):\n",
    "        x, y = X\n",
    "        all_x_i.append(x)\n",
    "        all_y_i.append(y)\n",
    "        all_f_i.append(f(X))\n",
    "\n",
    "    sp.optimize.minimize(\n",
    "        f, x0, method=\"Nelder-Mead\", callback=store, options={\"ftol\": 1e-12}\n",
    "    )\n",
    "    return all_x_i, all_y_i, all_f_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508d9e4",
   "metadata": {},
   "source": [
    "**Authors**: *GaÃ«l Varoquaux*\n",
    "\n",
    "[Mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization) deals with the\n",
    "problem of finding numerically minimums (or maximums or zeros) of\n",
    "a function. In this context, the function is called *cost function*, or\n",
    "*objective function*, or *energy*.\n",
    "\n",
    "Here, we are interested in using {mod}`scipy.optimize` for black-box\n",
    "optimization: we do not rely on the mathematical expression of the\n",
    "function that we are optimizing. Note that this expression can often be\n",
    "used for more efficient, non black-box, optimization.\n",
    "\n",
    "**Start of admonition: Prerequisites**\n",
    "\n",
    " * {ref}`NumPy <numpy>`\n",
    " * {ref}`SciPy <scipy>`\n",
    " * {ref}`Matplotlib <matplotlib>`\n",
    "\n",
    "**End of admonition**\n",
    "\n",
    "**Start of admonition: See also**\n",
    "\n",
    "**References**\n",
    "\n",
    "Mathematical optimization is very ... mathematical. If you want\n",
    "performance, it really pays to read the books:\n",
    "\n",
    "- [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)\n",
    "  by Boyd and Vandenberghe (pdf available free online).\n",
    "- [Numerical Optimization](https://users.eecs.northwestern.edu/~nocedal/book/num-opt.html),\n",
    "  by Nocedal and Wright. Detailed reference on gradient descent methods.\n",
    "- [Practical Methods of Optimization](https://www.amazon.com/gp/product/0471494631/ref=ox_sc_act_title_1?ie=UTF8&smid=ATVPDKIKX0DER) by Fletcher: good at hand-waving explanations.\n",
    "**End of admonition**\n",
    "\n",
    "<!---\n",
    "XXX: should I discuss root finding?\n",
    "-->\n",
    "\n",
    "## Knowing your problem\n",
    "\n",
    "Not all optimization problems are equal. Knowing your problem enables you\n",
    "to choose the right tool.\n",
    "\n",
    "**Start of admonition: Dimensionality of the problem**\n",
    "The scale of an optimization problem is pretty much set by the\n",
    "*dimensionality of the problem*, i.e. the number of scalar variables\n",
    "on which the search is performed.\n",
    "**End of admonition**\n",
    "\n",
    "### Convex versus non-convex optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9f6c6",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 2)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# A convex function\n",
    "plt.plot(x, x**2, linewidth=2)\n",
    "plt.text(-0.7, -(0.6**2), \"$f$\", size=20)\n",
    "\n",
    "# The tangent in one point\n",
    "plt.plot(x, 2 * x - 1)\n",
    "plt.plot(1, 1, \"k+\")\n",
    "plt.text(0.3, -0.75, \"Tangent to $f$\", size=15)\n",
    "plt.text(1, 1 - 0.5, \"C\", size=15)\n",
    "\n",
    "# Convexity as barycenter\n",
    "plt.plot([0.35, 1.85], [0.35**2, 1.85**2])\n",
    "plt.plot([0.35, 1.85], [0.35**2, 1.85**2], \"k+\")\n",
    "plt.text(0.35 - 0.2, 0.35**2 + 0.1, \"A\", size=15)\n",
    "plt.text(1.85 - 0.2, 1.85**2, \"B\", size=15)\n",
    "\n",
    "plt.ylim(ymin=-1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"A Convex Function\", fontstyle='italic')\n",
    "\n",
    "# Convexity as barycenter\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, x**2 + np.exp(-5 * (x - 0.5) ** 2), linewidth=2)\n",
    "plt.text(-0.7, -(0.6**2), \"$f$\", size=20)\n",
    "\n",
    "plt.ylim(ymin=-1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('A Non-convex Function', fontstyle='italic')\n",
    "\n",
    "caption_text_1=\"\"\"\n",
    "- $f$ is above all its tangents.\n",
    "- equivalently, for two points $A, B, f(C)$ lies below the segment\n",
    "$[f(A), f(B])], \\\\text{if } A < C < B $\n",
    "\"\"\"\n",
    "\n",
    "plt.figtext(0.25, -0.2, caption_text_1, wrap=True,\n",
    "            horizontalalignment='center',\n",
    "            fontsize=12)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e07ad5",
   "metadata": {},
   "source": [
    "**Optimizing convex functions is easy. Optimizing non-convex functions can\n",
    "be very hard.**\n",
    "\n",
    "**Start of note**\n",
    "It can be proven that for a convex function a local minimum is\n",
    "also a global minimum. Then, in some sense, the minimum is unique.\n",
    "**End of note**\n",
    "\n",
    "### Smooth and non-smooth problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc012c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "x = np.linspace(-1.5, 1.5, 101)\n",
    "\n",
    "# A smooth function\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(x, np.sqrt(0.2 + x**2), linewidth=2)\n",
    "plt.text(-1, 0, \"$f$\", size=20)\n",
    "plt.title('A Smooth Function',  fontstyle='italic')\n",
    "\n",
    "plt.ylim(ymin=-0.2)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# A non-smooth function\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, np.abs(x), linewidth=2)\n",
    "plt.text(-1, 0, \"$f$\", size=20)\n",
    "plt.title('A Non-smooth Function',  fontstyle='italic')\n",
    "\n",
    "plt.ylim(ymin=-0.2)\n",
    "caption_text_1=\"\"\"\n",
    "The gradient is defined everywhere, and is a continuous function.\n",
    "\"\"\"\n",
    "\n",
    "plt.figtext(0.25, -0.2, caption_text_1, wrap=True,\n",
    "            horizontalalignment='center',\n",
    "            fontsize=12)\n",
    "plt.tight_layout();\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28722e30",
   "metadata": {},
   "source": [
    "**Optimizing smooth functions is easier**\n",
    "(true in the context of *black-box* optimization, otherwise\n",
    "[Linear Programming](https://en.wikipedia.org/wiki/Linear_programming)\n",
    "is an example of methods which deal very efficiently with\n",
    "piece-wise linear functions).\n",
    "\n",
    "### Noisy versus exact cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990bbfb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "caption_text_1=\"\"\"\n",
    " Noisy (blue) and non-noisy (green) functions\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.3, 0.45, caption_text_1, wrap=True,\n",
    "            horizontalalignment='left',\n",
    "            fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "rng = np.random.default_rng(27446968)\n",
    "\n",
    "x = np.linspace(-5, 5, 101)\n",
    "x_ = np.linspace(-5, 5, 31)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return -np.exp(-(x**2))\n",
    "\n",
    "\n",
    "# A smooth function\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_, f(x_) + 0.2 * np.random.normal(size=31), linewidth=2)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "\n",
    "plt.ylim(ymin=-1.3)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c254d1",
   "metadata": {},
   "source": [
    "**Start of admonition: Noisy gradients**\n",
    "Many optimization methods rely on gradients of the objective function.\n",
    "If the gradient function is not given, they are computed numerically,\n",
    "which induces errors. In such situation, even if the objective\n",
    "function is not noisy, a gradient-based optimization may be a noisy\n",
    "optimization.\n",
    "**End of admonition**\n",
    "\n",
    "### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e675be",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "caption_text_1=\"\"\"\n",
    " - Optimizations under constraints\n",
    "\n",
    "    Here:\n",
    "\n",
    "     $-1 < x_1 < 1$\n",
    "\n",
    "     $-1 < x_2 < 1$\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.3, 0.45, caption_text_1, wrap=True,\n",
    "            horizontalalignment='left',\n",
    "            fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "x, y = np.mgrid[-2.9:5.8:0.05, -2.5:5:0.05]  # type: ignore[misc]\n",
    "x = x.T\n",
    "y = y.T\n",
    "\n",
    "for i in (1, 2):\n",
    "    # Create 2 figure: only the second one will have the optimization\n",
    "    # path\n",
    "    contours = plt.contour(\n",
    "        np.sqrt((x - 3) ** 2 + (y - 2) ** 2),\n",
    "        extent=[-3, 6, -2.5, 5],\n",
    "        cmap=\"gnuplot\",\n",
    "    )\n",
    "    plt.clabel(contours, inline=1, fmt=\"%1.1f\", fontsize=14)\n",
    "    plt.plot(\n",
    "        [-1.5, -1.5, 1.5, 1.5, -1.5], [-1.5, 1.5, 1.5, -1.5, -1.5], \"k\", linewidth=2\n",
    "    )\n",
    "    plt.fill_between([-1.5, 1.5], [-1.5, -1.5], [1.5, 1.5], color=\".8\")\n",
    "    plt.axvline(0, color=\"k\")\n",
    "    plt.axhline(0, color=\"k\")\n",
    "\n",
    "    plt.text(-0.9, 4.4, \"$x_2$\", size=20)\n",
    "    plt.text(5.6, -0.6, \"$x_1$\", size=20)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = []\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3) ** 2 + (x[1] - 2) ** 2)\n",
    "\n",
    "\n",
    "# We don't use the gradient, as with the gradient, L-BFGS is too fast,\n",
    "# and finds the optimum without showing us a pretty path\n",
    "def f_prime(x):\n",
    "    r = np.sqrt((x[0] - 3) ** 2 + (x[0] - 2) ** 2)\n",
    "    return np.array(((x[0] - 3) / r, (x[0] - 2) / r))\n",
    "\n",
    "\n",
    "sp.optimize.minimize(\n",
    "    f, np.array([0, 0]), method=\"L-BFGS-B\", bounds=((-1.5, 1.5), (-1.5, 1.5))\n",
    ")\n",
    "accumulated = np.array(accumulator)\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4667720",
   "metadata": {},
   "source": [
    "## A review of the different optimizers\n",
    "\n",
    "### Getting started: 1D optimization\n",
    "\n",
    "Let's get started by finding the minimum of the scalar function\n",
    "$f(x)=\\exp[(x-0.5)^2]$. {func}`scipy.optimize.minimize_scalar` uses\n",
    "Brent's method to find the minimum of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "def f(x):\n",
    "    return -np.exp(-(x - 0.5)**2)\n",
    "result = sp.optimize.minimize_scalar(f)\n",
    "result.success # check if solver was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = result.x\n",
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139821b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1edd79d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 3, 100)\n",
    "x_0 = np.exp(-1)\n",
    "\n",
    "def f(x):\n",
    "    return (x - x_0)**2 + epsilon*np.exp(-5*(x - .5 - x_0)**2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for epsilon in (0, 1):\n",
    "    if epsilon == 0:\n",
    "        subplot_n0 = 1\n",
    "        subplot_n1 = 2\n",
    "        subplot_n2 = 3\n",
    "    else:\n",
    "        subplot_n0 = 4\n",
    "        subplot_n1 = 5\n",
    "        subplot_n2 = 6\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    if epsilon == 0:\n",
    "        plt.text(-0.3, 1, \"Brentâs method on a quadratic function:\", fontweight='bold', horizontalalignment='left',\n",
    "            fontsize=12)\n",
    "        caption_text = \"This converges in 3 iterations, as the quadratic \\napproximation is then exact.\"\n",
    "        plt.text(-0.3, 0.83, caption_text,\n",
    "            horizontalalignment='left',\n",
    "            fontsize=12,\n",
    "            wrap=True)\n",
    "    else:\n",
    "        plt.text(-0.3, 1, \"Brentâs method on a non-convex function\", fontweight='bold', horizontalalignment='left',\n",
    "            fontsize=12)\n",
    "        caption_text = \"Note that the fact that the optimizer avoided\\nthe local minimum is a matter of luck.\"\n",
    "        plt.text(-0.3, 0.83, caption_text,\n",
    "            horizontalalignment='left',\n",
    "            fontsize=12,\n",
    "            wrap=True)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n1)\n",
    "\n",
    "    # A convex function\n",
    "    plt.plot(x, f(x), linewidth=2)\n",
    "\n",
    "    # Apply brent method. To have access to the iteration, do this in an\n",
    "    # artificial way: allow the algorithm to iter only once\n",
    "    all_x = list()\n",
    "    all_y = list()\n",
    "    for iter in range(30):\n",
    "        result = optimize.minimize_scalar(f, bracket=(-5, 2.9, 4.5), method=\"Brent\",\n",
    "                    options={\"maxiter\": iter}, tol=np.finfo(1.).eps)\n",
    "        if result.success:\n",
    "            print('Converged at ', iter)\n",
    "            break\n",
    "\n",
    "        this_x = result.x\n",
    "        all_x.append(this_x)\n",
    "        all_y.append(f(this_x))\n",
    "        if iter < 6:\n",
    "            plt.text(this_x - .05*np.sign(this_x) - .05,\n",
    "                    f(this_x) + 1.2*(.3 - iter % 2), iter + 1,\n",
    "                    size=12)\n",
    "\n",
    "    plt.plot(all_x[:10], all_y[:10], 'k+', markersize=12, markeredgewidth=2)\n",
    "    plt.plot(all_x[-1], all_y[-1], 'rx', markersize=12)\n",
    "    plt.ylim(ymin=-1, ymax=8)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n2)\n",
    "    plt.semilogy(np.abs(all_y - all_y[-1]), linewidth=2)\n",
    "    plt.ylabel('Error on f(x)')\n",
    "    plt.xlabel('Iteration')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23954756",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Start of note**\n",
    "You can use different solvers using the parameter `method`.\n",
    "**End of note**\n",
    "\n",
    "**Start of note**\n",
    "{func}`scipy.optimize.minimize_scalar` can also be used for optimization\n",
    "constrained to an interval using the parameter `bounds`.\n",
    "**End of note**\n",
    "\n",
    "\n",
    "### Gradient based methods\n",
    "\n",
    "#### Some intuitions about gradient descent\n",
    "\n",
    "Here we focus on **intuitions**, not code. Code will follow.\n",
    "\n",
    "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "basically consists in taking small steps in the direction of the\n",
    "gradient, that is the direction of the *steepest descent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f10b21",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x_min, x_max = -1, 2\n",
    "y_min, y_max = 2.25 / 3 * x_min - 0.2, 2.25 / 3 * x_max - 0.2\n",
    "\n",
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Fixed step gradient descent', fontweight='bold')\n",
    "plt.axis('off')\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (   (mk_quad(0.7), gradient_descent),\n",
    "        (mk_quad(0.02), gradient_descent),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    titles = [\"A well-conditioned quadratic function.\",\n",
    "              \"An ill-conditioned quadratic function.\"]\n",
    "\n",
    "    captions = [ \"\",\n",
    "    \"The core problem of gradient-methods on\\n ill-conditioned problems is that the gradient\\ntends not to point in the direction of the\\nminimum\"\n",
    "      ]\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    plt.text(-0.3, 1, titles[index], fontweight='bold', horizontalalignment='left',\n",
    "        fontsize=12)\n",
    "    caption_text = captions[index]\n",
    "    plt.text(-0.3, 0.6, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d146e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can see that very anisotropic ([ill-conditioned](https://en.wikipedia.org/wiki/Condition_number)) functions are harder\n",
    "to optimize.\n",
    "\n",
    "**Start of admonition: Take home message: conditioning number and preconditioning**\n",
    "If you know natural scaling for your variables, prescale them so that\n",
    "they behave similarly. This is related to [preconditioning](https://en.wikipedia.org/wiki/Preconditioner).\n",
    "**End of admonition**\n",
    "\n",
    "Also, it clearly can be advantageous to take bigger steps. This\n",
    "is done in gradient descent code using a\n",
    "[line search](https://en.wikipedia.org/wiki/Line_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc7363",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x_min, x_max = -1, 2\n",
    "y_min, y_max = 2.25 / 3 * x_min - 0.2, 2.25 / 3 * x_max - 0.2\n",
    "\n",
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Adaptive step gradient descent', fontweight='bold')\n",
    "plt.axis('off')\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        (mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        (mk_quad(0.02), gradient_descent_adaptative),\n",
    "        (mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "            gradient_descent_adaptative,),\n",
    "        #(mk_gauss(0.02), conjugate_gradient),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        #(mk_quad(0.02), newton_cg),\n",
    "        #(mk_gauss(0.02), newton_cg),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        #(mk_quad(0.02), bfgs),\n",
    "        #(mk_gauss(0.02), bfgs),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        #(mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        #(mk_gauss(0.02), nelder_mead),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    # titles = []\n",
    "\n",
    "    captions = [\"A well-conditioned quadratic function.\",\n",
    "                \"An ill-conditioned quadratic function.\",\n",
    "                \"An ill-conditioned non-quadratic function.\",\n",
    "                \"An ill-conditioned very non-quadratic function.\"]\n",
    "\n",
    "    plt.subplot(4, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    #plt.text(-0.3, 1, titles[row], fontweight='bold', horizontalalignment='left',\n",
    "        #fontsize=12)\n",
    "    caption_text = captions[row-1]\n",
    "    plt.text(-0.3, 0.83, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(4, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(4, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba1dad",
   "metadata": {},
   "source": [
    "The more a function looks like a quadratic function (elliptic\n",
    "iso-curves), the easier it is to optimize.\n",
    "\n",
    "#### Conjugate gradient descent\n",
    "\n",
    "The gradient descent algorithms above are toys not to be used on real\n",
    "problems.\n",
    "\n",
    "As can be seen from the above experiments, one of the problems of the\n",
    "simple gradient descent algorithms, is that it tends to oscillate across\n",
    "a valley, each time following the direction of the gradient, that makes\n",
    "it cross the valley. The conjugate gradient solves this problem by adding\n",
    "a *friction* term: each step depends on the two last values of the\n",
    "gradient and sharp turns are reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d95ef",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x_min, x_max = -1, 2\n",
    "y_min, y_max = 2.25 / 3 * x_min - 0.2, 2.25 / 3 * x_max - 0.2\n",
    "\n",
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Conjugate gradient descent', fontweight='bold')\n",
    "plt.axis('off')\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        #(mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        #(mk_quad(0.02), gradient_descent_adaptative),\n",
    "        #(mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "            #gradient_descent_adaptative,),\n",
    "        (mk_gauss(0.02), conjugate_gradient),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        #(mk_quad(0.02), newton_cg),\n",
    "        #(mk_gauss(0.02), newton_cg),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        #(mk_quad(0.02), bfgs),\n",
    "        #(mk_gauss(0.02), bfgs),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        #(mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        #(mk_gauss(0.02), nelder_mead),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    # titles = []\n",
    "\n",
    "    captions = [\"A well-conditioned quadratic function.\",\n",
    "                \"An ill-conditioned quadratic function.\",\n",
    "                \"An ill-conditioned non-quadratic function.\",\n",
    "                \"An ill-conditioned very non-quadratic function.\"]\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    #plt.text(-0.3, 1, titles[row], fontweight='bold', horizontalalignment='left',\n",
    "        #fontsize=12)\n",
    "    caption_text = captions[row-1]\n",
    "    plt.text(-0.3, 0.83, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89344652",
   "metadata": {},
   "source": [
    "SciPy provides {func}`scipy.optimize.minimize` to find the minimum of scalar\n",
    "functions of one or more variables. The simple conjugate gradient method can\n",
    "be used by setting the parameter `method` to CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "sp.optimize.minimize(f, [2, -1], method=\"CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc133c",
   "metadata": {},
   "source": [
    "Gradient methods need the Jacobian (gradient) of the function. They can compute it\n",
    "numerically, but will perform better if you can pass them the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cca455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "sp.optimize.minimize(f, [2, 1], method=\"CG\", jac=jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426389a",
   "metadata": {},
   "source": [
    "Note that the function has only been evaluated 27 times, compared to 108\n",
    "without the gradient.\n",
    "\n",
    "### Newton and quasi-newton methods\n",
    "\n",
    "#### Newton methods: using the Hessian (2nd differential)\n",
    "\n",
    "[Newton methods](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) use a\n",
    "local quadratic approximation to compute the jump direction. For this\n",
    "purpose, they rely on the 2 first derivative of the function: the\n",
    "*gradient* and the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17d2f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        #(mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        #(mk_quad(0.02), gradient_descent_adaptative),\n",
    "        #(mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "           # gradient_descent_adaptative,),\n",
    "        #(mk_gauss(0.02), conjugate_gradient),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        (mk_quad(0.02), newton_cg),\n",
    "        (mk_gauss(0.02), newton_cg),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        #(mk_quad(0.02), bfgs),\n",
    "        #(mk_gauss(0.02), bfgs),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        #(mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        #(mk_gauss(0.02), nelder_mead),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    titles = [\"An ill-conditioned quadratic function:\",\n",
    "             \"An ill-conditioned quadratic function:\",\n",
    "              \"An ill-conditioned very non-quadratic \\nfunction:\"]\n",
    "\n",
    "    captions = [\"Note that, as the quadratic\\napproximation is exact, the Newton\\nmethod is blazing fast\",\n",
    "               \"Here we are optimizing a\\nGaussian, which is always below\\nits quadratic approximation. As a\\nresult, the Newton method \\novershoots and leads to oscillations.\",\n",
    "               \"\"]\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    plt.text(-0.3, 1, titles[row-1], fontweight='bold', horizontalalignment='left',\n",
    "        fontsize=12)\n",
    "    caption_text = captions[row-1]\n",
    "    plt.text(-0.3, 0.5, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e89880",
   "metadata": {},
   "source": [
    "In SciPy, you can use the Newton method by setting `method` to Newton-CG in\n",
    "{func}`scipy.optimize.minimize`. Here, CG refers to the fact that an internal\n",
    "inversion of the Hessian is performed by conjugate gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d144f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "def jacobian(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "sp.optimize.minimize(f, [2,-1], method=\"Newton-CG\", jac=jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b156b6",
   "metadata": {},
   "source": [
    "Note that compared to a conjugate gradient (above), Newton's method has\n",
    "required less function evaluations, but more gradient evaluations, as it\n",
    "uses it to approximate the Hessian. Let's compute the Hessian and pass it\n",
    "to the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d49c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x): # Computed with sympy\n",
    "    return np.array(((1 - 4*x[1] + 12*x[0]**2, -4*x[0]), (-4*x[0], 2)))\n",
    "\n",
    "sp.optimize.minimize(f, [2,-1], method=\"Newton-CG\", jac=jacobian, hess=hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7cb7c",
   "metadata": {},
   "source": [
    "**Start of note**\n",
    "At very high-dimension, the inversion of the Hessian can be costly\n",
    "and unstable (large scale > 250).\n",
    "**End of note**\n",
    "\n",
    "**Start of note**\n",
    "Newton optimizers should not to be confused with Newton's root finding\n",
    "method, based on the same principles, {func}`scipy.optimize.newton`.\n",
    "**End of note**\n",
    "#### Quasi-Newton methods: approximating the Hessian on the fly\n",
    "\n",
    "**BFGS**: BFGS (Broyden-Fletcher-Goldfarb-Shanno algorithm) refines at\n",
    "each step an approximation of the Hessian.\n",
    "\n",
    "## Full code examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0607c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        #(mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        #(mk_quad(0.02), gradient_descent_adaptative),\n",
    "        #(mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "           # gradient_descent_adaptative,),\n",
    "        #(mk_gauss(0.02), conjugate_gradient),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        #(mk_quad(0.02), newton_cg),\n",
    "        #(mk_gauss(0.02), newton_cg),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        (mk_quad(0.02), bfgs),\n",
    "        (mk_gauss(0.02), bfgs),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        #(mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        #(mk_gauss(0.02), nelder_mead),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    titles = [\"An ill-conditioned quadratic function:\",\n",
    "              \"An ill-conditioned non-quadratic function:\",\n",
    "              \"An ill-conditioned very non-quadratic function:\"]\n",
    "\n",
    "    captions = [\"\\nAn ill-conditioned quadratic function: On an \\nexactly quadratic function, BFGS is not as fast\\nas Newtonâs method, but still very fast.\",\n",
    "                \"\\n\\nHere BFGS does better than Newton, as its\\nempirical estimate of the curvature is better than\\nthat given by the Hessian.\",\n",
    "                \"\"]\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    plt.text(-0.3, 1, titles[index], fontweight='bold', horizontalalignment='left',\n",
    "        fontsize=12)\n",
    "    caption_text = captions[index]\n",
    "    plt.text(-0.3, 0.7, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(3, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "def jacobian(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "sp.optimize.minimize(f, [2, -1], method=\"BFGS\", jac=jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da5225",
   "metadata": {},
   "source": [
    "**L-BFGS:** Limited-memory BFGS Sits between BFGS and conjugate gradient:\n",
    "in very high dimensions (> 250) the Hessian matrix is too costly to\n",
    "compute and invert. L-BFGS keeps a low-rank version. In addition, box bounds\n",
    "are also supported by L-BFGS-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "def jacobian(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "\n",
    "sp.optimize.minimize(f, [2, 2], method=\"L-BFGS-B\", jac=jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8caae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Gradient-less methods\n",
    "\n",
    "#### A shooting method: the Powell algorithm\n",
    "\n",
    "Almost a gradient approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46c22b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Powell's method\", fontweight='bold')\n",
    "plt.axis('off')\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        #(mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        #(mk_quad(0.02), gradient_descent_adaptative),\n",
    "        #(mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "            #gradient_descent_adaptative,),\n",
    "        #(mk_gauss(0.02), conjugate_gradient),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        #(mk_quad(0.02), newton_cg),\n",
    "        #(mk_gauss(0.02), newton_cg),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        #(mk_quad(0.02), bfgs),\n",
    "        #(mk_gauss(0.02), bfgs),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        (mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        #(mk_gauss(0.02), nelder_mead),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    titles = [\"An ill-conditioned quadratic function:\",\n",
    "             \"An ill-conditioned very non-quadratic function:\"]\n",
    "\n",
    "    captions = [\"Powellâs method isnât too sensitive to local \\nill-conditionning in low dimensions.\",\n",
    "                \"\"]\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    plt.text(-0.3, 1, titles[index], fontweight='bold', horizontalalignment='left',\n",
    "        fontsize=12)\n",
    "    caption_text = captions[index]\n",
    "    plt.text(-0.3, 0.83, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefeca0",
   "metadata": {},
   "source": [
    "#### Simplex method: the Nelder-Mead\n",
    "\n",
    "The Nelder-Mead algorithms is a generalization of dichotomy approaches to\n",
    "high-dimensional spaces. The algorithm works by refining a [simplex](https://en.wikipedia.org/wiki/Simplex), the generalization of intervals\n",
    "and triangles to high-dimensional spaces, to bracket the minimum.\n",
    "\n",
    "**Strong points**: it is robust to noise, as it does not rely on\n",
    "computing gradients. Thus it can work on functions that are not locally\n",
    "smooth such as experimental data points, as long as they display a\n",
    "large-scale bell-shape behavior. However it is slower than gradient-based\n",
    "methods on smooth, non-noisy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1a4ca",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "levels = {}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Simplex method: the Nelder-Mead\", fontweight='bold')\n",
    "plt.axis('off')\n",
    "for index, ((f, f_prime, hessian), optimizer) in enumerate(\n",
    "    (\n",
    "        #(mk_quad(0.7), gradient_descent),\n",
    "        #(mk_quad(0.7), gradient_descent_adaptative),\n",
    "        #(mk_quad(0.02), gradient_descent),\n",
    "        #(mk_quad(0.02), gradient_descent_adaptative),\n",
    "        #(mk_gauss(0.02), gradient_descent_adaptative),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n",
    "            #gradient_descent_adaptative,),\n",
    "        #(mk_gauss(0.02), conjugate_gradient),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n",
    "        #(mk_quad(0.02), newton_cg),\n",
    "        #(mk_gauss(0.02), newton_cg),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n",
    "        #(mk_quad(0.02), bfgs),\n",
    "        #(mk_gauss(0.02), bfgs),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n",
    "        #(mk_quad(0.02), powell),\n",
    "        #(mk_gauss(0.02), powell),\n",
    "        #((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n",
    "        (mk_gauss(0.02), nelder_mead),\n",
    "        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n",
    "    )\n",
    "):\n",
    "    # Compute a gradient-descent\n",
    "    x_i, y_i = 1.6, 1.1\n",
    "    counting_f_prime = CountingFunction(f_prime)\n",
    "    counting_hessian = CountingFunction(hessian)\n",
    "    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n",
    "    all_x_i, all_y_i, all_f_i = optimizer(\n",
    "        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n",
    "    )\n",
    "\n",
    "    row = index+1\n",
    "\n",
    "    subplot_n0, subplot_n1, subplot_n2 = get_subplot_n(index)\n",
    "\n",
    "    titles = [\"An ill-conditioned non-quadratic function:\",\n",
    "             \"An ill-conditioned very non-quadratic function:\"]\n",
    "\n",
    "    captions = [\"\",\n",
    "                \"\"]\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n0)\n",
    "    plt.scatter([0, 1], [0, 1], c='white')\n",
    "    plt.axis('off')\n",
    "    plt.text(-0.3, 1, titles[index], fontweight='bold', horizontalalignment='left',\n",
    "        fontsize=12)\n",
    "    caption_text = captions[index]\n",
    "    plt.text(-0.3, 0.83, caption_text,\n",
    "        horizontalalignment='left',\n",
    "        fontsize=12,\n",
    "        wrap=True)\n",
    "\n",
    "    if not max(all_y_i) < y_max:\n",
    "        x_min *= 1.2\n",
    "        x_max *= 1.2\n",
    "        y_min *= 1.2\n",
    "        y_max *= 1.2\n",
    "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "\n",
    "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
    "    z = np.apply_along_axis(f, 0, X)\n",
    "    log_z = np.log(z + 0.01)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n1)\n",
    "    plt.imshow(\n",
    "        log_z,\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gray_r,\n",
    "        origin=\"lower\",\n",
    "        vmax=log_z.min() + 1.5 * np.ptp(log_z),\n",
    "    )\n",
    "    contours = plt.contour(\n",
    "        log_z,\n",
    "        levels=levels.get(f),\n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap=plt.cm.gnuplot,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    levels[f] = contours.levels\n",
    "    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n",
    "\n",
    "    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n",
    "    plt.plot(all_x_i, all_y_i, \"k+\")\n",
    "\n",
    "    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n",
    "\n",
    "    plt.plot([0], [0], \"rx\", markersize=12)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.subplot(2, 3, subplot_n2)\n",
    "    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n",
    "    plt.ylabel(\"Error on f(x)\")\n",
    "    plt.semilogy(\n",
    "        logging_f.counts,\n",
    "        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n",
    "        linewidth=2,\n",
    "        color=\"g\",\n",
    "        label=\"# function calls\",\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        frameon=True,\n",
    "        prop={\"size\": 11},\n",
    "        borderaxespad=0,\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.5,\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca738667",
   "metadata": {},
   "source": [
    "Using the Nelder-Mead solver in {func}`scipy.optimize.minimize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ba3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "sp.optimize.minimize(f, [2, -1], method=\"Nelder-Mead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70df21",
   "metadata": {},
   "source": [
    "### Global optimizers\n",
    "\n",
    "If your problem does not admit a unique local minimum (which can be hard\n",
    "to test unless the function is convex), and you do not have prior\n",
    "information to initialize the optimization close to the solution, you\n",
    "may need a global optimizer.\n",
    "\n",
    "#### Brute force: a grid search\n",
    "\n",
    "{func}`scipy.optimize.brute` evaluates the function on a given grid of\n",
    "parameters and returns the parameters corresponding to the minimum\n",
    "value. The parameters are specified with ranges given to\n",
    "{obj}`numpy.mgrid`. By default, 20 steps are taken in each direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "sp.optimize.brute(f, ((-1, 2), (-1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46931c",
   "metadata": {},
   "source": [
    "## Practical guide to optimization with SciPy\n",
    "\n",
    "### Choosing a method\n",
    "\n",
    "All methods are exposed as the `method` argument of\n",
    "{func}`scipy.optimize.minimize`.\n",
    "\n",
    "<!---\n",
    "![](auto_examples/images/sphx_glr_plot_compare_optimizers_001.png)\n",
    "# For parsing.\n",
    ":align: center\n",
    ":width: 95%\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d507c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting the comparison of optimizers\n",
    "======================================\n",
    "\n",
    "Plots the results from the comparison of optimizers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = pickle.load(\n",
    "    open(f\"helper/compare_optimizers_py{sys.version_info[0]}.pkl\", \"rb\")\n",
    ")\n",
    "n_methods = len(list(results.values())[0][\"Rosenbrock  \"])\n",
    "n_dims = len(results)\n",
    "\n",
    "symbols = \"o>*Ds\"\n",
    "\n",
    "plt.figure(1, figsize=(10, 4))\n",
    "plt.clf()\n",
    "\n",
    "nipy_spectral = matplotlib.colormaps[\"nipy_spectral\"]\n",
    "colors = nipy_spectral(np.linspace(0, 1, n_dims))[:, :3]\n",
    "\n",
    "method_names = list(list(results.values())[0][\"Rosenbrock  \"].keys())\n",
    "method_names.sort(key=lambda x: x[::-1], reverse=True)\n",
    "\n",
    "for n_dim_index, ((n_dim, n_dim_bench), color) in enumerate(\n",
    "    zip(sorted(results.items()), colors, strict=True)\n",
    "):\n",
    "    for (cost_name, cost_bench), symbol in zip(\n",
    "        sorted(n_dim_bench.items()), symbols, strict=True\n",
    "    ):\n",
    "        for (\n",
    "            method_index,\n",
    "            method_name,\n",
    "        ) in enumerate(method_names):\n",
    "            this_bench = cost_bench[method_name]\n",
    "            bench = np.mean(this_bench)\n",
    "            plt.semilogy(\n",
    "                [\n",
    "                    method_index + 0.1 * n_dim_index,\n",
    "                ],\n",
    "                [\n",
    "                    bench,\n",
    "                ],\n",
    "                marker=symbol,\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "# Create a legend for the problem type\n",
    "for cost_name, symbol in zip(sorted(n_dim_bench.keys()), symbols, strict=True):\n",
    "    plt.semilogy(\n",
    "        [\n",
    "            -10,\n",
    "        ],\n",
    "        [\n",
    "            0,\n",
    "        ],\n",
    "        symbol,\n",
    "        color=\".5\",\n",
    "        label=cost_name,\n",
    "    )\n",
    "\n",
    "plt.xticks(np.arange(n_methods), method_names, size=11)\n",
    "plt.xlim(-0.2, n_methods - 0.5)\n",
    "plt.legend(loc=\"best\", numpoints=1, handletextpad=0, prop={\"size\": 12}, frameon=False)\n",
    "plt.ylabel(\"# function calls (a.u.)\")\n",
    "\n",
    "# Create a second legend for the problem dimensionality\n",
    "plt.twinx()\n",
    "\n",
    "for n_dim, color in zip(sorted(results.keys()), colors, strict=True):\n",
    "    plt.plot(\n",
    "        [\n",
    "            -10,\n",
    "        ],\n",
    "        [\n",
    "            0,\n",
    "        ],\n",
    "        \"o\",\n",
    "        color=color,\n",
    "        label=f\"# dim: {n_dim}\",\n",
    "    )\n",
    "plt.legend(\n",
    "    loc=(0.47, 0.07),\n",
    "    numpoints=1,\n",
    "    handletextpad=0,\n",
    "    prop={\"size\": 12},\n",
    "    frameon=False,\n",
    "    ncol=2,\n",
    ")\n",
    "plt.xlim(-0.2, n_methods - 0.5)\n",
    "\n",
    "plt.xticks(np.arange(n_methods), method_names)\n",
    "plt.yticks(())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0164f0",
   "metadata": {},
   "source": [
    "**With knowledge of the gradient**\n",
    "\n",
    "* **BFGS** or **L-BFGS**.\n",
    "* Computational overhead of BFGS is larger than that L-BFGS, itself\n",
    "  larger than that of conjugate gradient. On the other side, BFGS usually\n",
    "  needs less function evaluations than CG. Thus conjugate gradient method\n",
    "  is better than BFGS at optimizing computationally cheap functions.\n",
    "\n",
    "**With the Hessian**\n",
    "\n",
    "* If you can compute the Hessian, prefer the Newton method\n",
    "  (**Newton-CG** or **TCG**).\n",
    "\n",
    "**If you have noisy measurements**\n",
    "\n",
    "* Use **Nelder-Mead** or **Powell**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71dc51",
   "metadata": {},
   "source": [
    "### Making your optimizer faster\n",
    "\n",
    "- Choose the right method (see above), do compute analytically the\n",
    "  gradient and Hessian, if you can.\n",
    "- Use [preconditionning](https://en.wikipedia.org/wiki/Preconditioner)\n",
    "  when possible.\n",
    "- Choose your initialization points wisely. For instance, if you are\n",
    "  running many similar optimizations, warm-restart one with the results of\n",
    "  another.\n",
    "- Relax the tolerance if you don't need precision using the parameter `tol`.\n",
    "\n",
    "### Computing gradients\n",
    "\n",
    "Computing gradients, and even more Hessians, is very tedious but worth\n",
    "the effort. Symbolic computation with {ref}`Sympy <sympy>` may come in\n",
    "handy.\n",
    "\n",
    "**Warning**\n",
    "\n",
    "A *very* common source of optimization not converging well is human\n",
    "error in the computation of the gradient. You can use\n",
    "{func}`scipy.optimize.check_grad` to check that your gradient is\n",
    "correct. It returns the norm of the different between the gradient\n",
    "given, and a gradient computed numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.optimize.check_grad(f, jacobian, [2, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504c0d2",
   "metadata": {},
   "source": [
    "See also {func}`scipy.optimize.approx_fprime` to find your errors.\n",
    "\n",
    "### Synthetic exercises\n",
    "\n",
    "**A simple (?) quadratic function**\n",
    "\n",
    "**Start of exercise**\n",
    "\n",
    "Optimize the following function, using K[0] as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)\n",
    "K = rng.normal(size=(100, 100))\n",
    "\n",
    "def f(x):\n",
    "    return np.sum((K @ (x - 1))**2) + np.sum(x**2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58fa82",
   "metadata": {},
   "source": [
    "Time your approach. Find the fastest approach. Why is BFGS not\n",
    "working well?\n",
    "\n",
    "**End of exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d848d",
   "metadata": {},
   "source": [
    "**See the [corresponding page](/scipy-lecture-notes/advanced/mathematical_optimization/index.html) for solution**\n",
    "\n",
    "**A locally flat minimum**\n",
    "\n",
    "**Start of exercise**\n",
    "\n",
    "Consider the function `exp(-1/(.1*x**2 + y**2)`. This function admits\n",
    "a minimum in (0, 0). Starting from an initialization at (1, 1), try\n",
    "to get within 1e-8 of this minimum point.\n",
    "\n",
    "This exercise is hard because the function is very flat around the minimum\n",
    "(all its derivatives are zero). Thus gradient information is unreliable.\n",
    "\n",
    "**End of exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb8728",
   "metadata": {},
   "source": [
    "**See the [corresponding page](/scipy-lecture-notes/advanced/mathematical_optimization/index.html) for solution**\n",
    "\n",
    "## Special case: non-linear least-squares\n",
    "\n",
    "### Minimizing the norm of a vector function\n",
    "\n",
    "Least square problems, minimizing the norm of a vector function, have a\n",
    "specific structure that can be used in the [LevenbergâMarquardt algorithm](https://en.wikipedia.org/wiki/Levenberg-Marquardt_algorithm)\n",
    "implemented in {func}`scipy.optimize.leastsq`.\n",
    "\n",
    "Lets try to minimize the norm of the following vectorial function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af298147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.arctan(x) - np.arctan(np.linspace(0, 1, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802db3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(10)\n",
    "sp.optimize.leastsq(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8753e5",
   "metadata": {},
   "source": [
    "This took 67 function evaluations (check it with 'full_output=True'). What\n",
    "if we compute the norm ourselves and use a good generic optimizer (BFGS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return np.sum(f(x)**2)\n",
    "result = sp.optimize.minimize(g, x0, method=\"BFGS\")\n",
    "result.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08d22b",
   "metadata": {},
   "source": [
    "BFGS needs more function calls, and gives a less precise result.\n",
    "\n",
    "**Start of note**\n",
    "`leastsq` is interesting compared to BFGS only if the\n",
    "dimensionality of the output vector is large, and larger than the number\n",
    "of parameters to optimize.\n",
    "**End of note**\n",
    "\n",
    "**Start of warning**\n",
    "If the function is linear, this is a linear-algebra problem, and\n",
    "should be solved with {func}`scipy.linalg.lstsq`.\n",
    "**End of warning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab822535",
   "metadata": {},
   "source": [
    "### Curve fitting\n",
    "\n",
    "Least square problems occur often when fitting a non-linear to data.\n",
    "While it is possible to construct our optimization problem ourselves,\n",
    "SciPy provides a helper function for this purpose:\n",
    "{func}`scipy.optimize.curve_fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa074d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(t, omega, phi):\n",
    "    return np.cos(omega * t + phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eade7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 50)\n",
    "rng = np.random.default_rng(27446968)\n",
    "y = f(x, 1.5, 1) + .1*rng.normal(size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.optimize.curve_fit(f, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f2f89",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(27446968)\n",
    "\n",
    "\n",
    "# Our test function\n",
    "def f(t, omega, phi):\n",
    "    return np.cos(omega * t + phi)\n",
    "\n",
    "\n",
    "# Our x and y data\n",
    "x = np.linspace(0, 3, 50)\n",
    "y = f(x, 1.5, 1) + 0.1 * np.random.normal(size=50)\n",
    "\n",
    "# Fit the model: the parameters omega and phi can be found in the\n",
    "# `params` vector\n",
    "params, params_cov = sp.optimize.curve_fit(f, x, y)\n",
    "\n",
    "# plot the data and the fitted curve\n",
    "t = np.linspace(0, 3, 1000)\n",
    "\n",
    "plt.plot(x, y, \"bx\")\n",
    "plt.plot(t, f(t, *params), \"r-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8b792",
   "metadata": {},
   "source": [
    "**Start of exercise**\n",
    "\n",
    "Do the same with omega = 3. What is the difficulty?\n",
    "\n",
    "**End of exercise**\n",
    "\n",
    "## Optimization with constraints\n",
    "\n",
    "### Box bounds\n",
    "\n",
    "Box bounds correspond to limiting each of the individual parameters of\n",
    "the optimization. Note that some problems that are not originally written\n",
    "as box bounds can be rewritten as such via change of variables. Both\n",
    "{func}`scipy.optimize.minimize_scalar` and {func}`scipy.optimize.minimize`\n",
    "support bound constraints with the parameter `bounds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sqrt((x[0] - 3)**2 + (x[1] - 2)**2)\n",
    "\n",
    "sp.optimize.minimize(f, np.array([0, 0]), bounds=((-1.5, 1.5), (-1.5, 1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc877b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-2.9:5.8:0.05, -2.5:5:0.05]  # type: ignore[misc]\n",
    "x = x.T\n",
    "y = y.T\n",
    "\n",
    "for i in (1, 2):\n",
    "    # Create 2 figure: only the second one will have the optimization\n",
    "    # path\n",
    "    if i == 2:\n",
    "        plt.figure(i, figsize=(3, 2.5))\n",
    "        plt.clf()\n",
    "        plt.axes((0, 0, 1, 1))\n",
    "\n",
    "        contours = plt.contour(\n",
    "            np.sqrt((x - 3) ** 2 + (y - 2) ** 2),\n",
    "            extent=[-3, 6, -2.5, 5],\n",
    "            cmap=\"gnuplot\",\n",
    "        )\n",
    "        plt.clabel(contours, inline=1, fmt=\"%1.1f\", fontsize=14)\n",
    "        plt.plot(\n",
    "            [-1.5, -1.5, 1.5, 1.5, -1.5], [-1.5, 1.5, 1.5, -1.5, -1.5], \"k\", linewidth=2\n",
    "        )\n",
    "        plt.fill_between([-1.5, 1.5], [-1.5, -1.5], [1.5, 1.5], color=\".8\")\n",
    "        plt.axvline(0, color=\"k\")\n",
    "        plt.axhline(0, color=\"k\")\n",
    "\n",
    "        plt.text(-0.9, 4.4, \"$x_2$\", size=20)\n",
    "        plt.text(5.6, -0.6, \"$x_1$\", size=20)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = []\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3) ** 2 + (x[1] - 2) ** 2)\n",
    "\n",
    "\n",
    "# We don't use the gradient, as with the gradient, L-BFGS is too fast,\n",
    "# and finds the optimum without showing us a pretty path\n",
    "def f_prime(x):\n",
    "    r = np.sqrt((x[0] - 3) ** 2 + (x[0] - 2) ** 2)\n",
    "    return np.array(((x[0] - 3) / r, (x[0] - 2) / r))\n",
    "\n",
    "\n",
    "sp.optimize.minimize(\n",
    "    f, np.array([0, 0]), method=\"L-BFGS-B\", bounds=((-1.5, 1.5), (-1.5, 1.5))\n",
    ")\n",
    "\n",
    "accumulated = np.array(accumulator)\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81fbbe",
   "metadata": {},
   "source": [
    "<!---\n",
    "![](auto_examples/images/sphx_glr_plot_constraints_002.png)\n",
    "# For parsing.\n",
    ":align: right\n",
    ":scale: 75%\n",
    ":target: auto_examples/plot_constraints.html\n",
    "-->\n",
    "\n",
    "### General constraints\n",
    "\n",
    "Equality and inequality constraints specified as functions: $f(x) = 0$\n",
    "and $g(x) < 0$.\n",
    "\n",
    "#### {func}`scipy.optimize.fmin_slsqp` Sequential least square programming:\n",
    "equality and inequality constraints:\n",
    "\n",
    "<!---\n",
    "![](auto_examples/images/sphx_glr_plot_non_bounds_constraints_001.png)\n",
    "# For parsing.\n",
    ":align: right\n",
    ":scale: 75%\n",
    ":target: auto_examples/plot_non_bounds_constraints.html\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f519b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sqrt((x[0] - 3)**2 + (x[1] - 2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044209d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint(x):\n",
    "    return np.atleast_1d(1.5 - np.sum(np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46204a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0, 0])\n",
    "sp.optimize.minimize(f, x0, constraints={\"fun\": constraint, \"type\": \"ineq\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad571681",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "x, y = np.mgrid[-2.03:4.2:0.04, -1.6:3.2:0.04]  # type: ignore[misc]\n",
    "x = x.T\n",
    "y = y.T\n",
    "\n",
    "plt.figure(1, figsize=(3, 2.5))\n",
    "plt.clf()\n",
    "plt.axes((0, 0, 1, 1))\n",
    "\n",
    "contours = plt.contour(\n",
    "    np.sqrt((x - 3) ** 2 + (y - 2) ** 2),\n",
    "    extent=[-2.03, 4.2, -1.6, 3.2],\n",
    "    cmap=\"gnuplot\",\n",
    ")\n",
    "plt.clabel(contours, inline=1, fmt=\"%1.1f\", fontsize=14)\n",
    "plt.plot([-1.5, 0, 1.5, 0, -1.5], [0, 1.5, 0, -1.5, 0], \"k\", linewidth=2)\n",
    "plt.fill_between([-1.5, 0, 1.5], [0, -1.5, 0], [0, 1.5, 0], color=\".8\")\n",
    "plt.axvline(0, color=\"k\")\n",
    "plt.axhline(0, color=\"k\")\n",
    "\n",
    "plt.text(-0.9, 2.8, \"$x_2$\", size=20)\n",
    "plt.text(3.6, -0.6, \"$x_1$\", size=20)\n",
    "plt.axis(\"tight\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = []\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3) ** 2 + (x[1] - 2) ** 2)\n",
    "\n",
    "\n",
    "def constraint(x):\n",
    "    return np.atleast_1d(1.5 - np.sum(np.abs(x)))\n",
    "\n",
    "\n",
    "sp.optimize.minimize(\n",
    "    f, np.array([0, 0]), method=\"SLSQP\", constraints={\"fun\": constraint, \"type\": \"ineq\"}\n",
    ")\n",
    "\n",
    "accumulated = np.array(accumulator)\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de625f",
   "metadata": {},
   "source": [
    "**Start of warning**\n",
    "The above problem is known as the [Lasso](<https://en.wikipedia.org/wiki/Lasso_(statistics)>)\n",
    "problem in statistics, and there exist very efficient solvers for it\n",
    "(for instance in [scikit-learn](https://scikit-learn.org)). In\n",
    "general do not use generic solvers when specific ones exist.\n",
    "**End of warning**\n",
    "\n",
    "**Start of admonition: Lagrange multipliers**\n",
    "If you are ready to do a bit of math, many constrained optimization\n",
    "problems can be converted to non-constrained optimization problems\n",
    "using a mathematical trick known as [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier).\n",
    "**End of admonition**\n",
    "\n",
    "## Full code examples\n",
    "\n",
    "<!---\n",
    "include the gallery. Skip the first line to avoid the \"orphan\"\n",
    "declaration\n",
    "-->\n",
    "\n",
    "**Start of admonition: See also**\n",
    "\n",
    "**Other Software**\n",
    "\n",
    "SciPy tries to include the best well-established, general-use,\n",
    "and permissively-licensed optimization algorithms available. However,\n",
    "even better options for a given task may be available in other libraries;\n",
    "please also see [IPOPT] and [PyGMO].\n",
    "**End of admonition**\n",
    "\n",
    "[ipopt]: https://github.com/xuy/pyipopt\n",
    "[pygmo]: https://esa.github.io/pygmo2/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
